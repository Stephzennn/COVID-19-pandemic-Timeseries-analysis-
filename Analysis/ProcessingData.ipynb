{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6c0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import yaml\n",
    "\n",
    "\"\"\"\n",
    "Converts multi-state COVID-19 cumulative data into weekly new cases/deaths,\n",
    "pivots to wide format, performs train/test split, and visualizes the results.\n",
    "\"\"\"\n",
    "# ============================================\n",
    "# CONFIGURATION & Dynamic Splitting Function\n",
    "# ============================================\n",
    "\n",
    "CONFIG = {\n",
    "    'input_file': 'FL_GA_NY.csv',\n",
    "    'test_weeks': 16,  # Last 16 weeks for testing\n",
    "    'date_column': 'Date',\n",
    "    'state_column': 'Province_State',\n",
    "    'confirmed_column': 'Confirmed',\n",
    "    'deaths_column': 'Deaths',\n",
    "    'claims_column': 'claims', \n",
    "    'recovered_column': 'Recovered', \n",
    "    'active_column': 'Active',  \n",
    "    'output_wide': 'multi_state_panel_wide.csv',\n",
    "    'output_train_wide': 'train_data.csv',\n",
    "    'output_test_wide': 'test_data.csv',\n",
    "    'output_train_long': 'train_long.csv',\n",
    "    'output_test_long': 'test_long.csv',\n",
    "    'output_viz': 'train_test_split_visualization.png',\n",
    "    'output_train_nf': 'train_neuralforecast.csv',\n",
    "    'output_test_nf': 'test_neuralforecast.csv'\n",
    "\n",
    "}\n",
    "\n",
    "# ============================================\n",
    "# LOAD DATA & DYNAMIC MULTI-STATE DATA PROCESSING\n",
    "# ============================================\n",
    "\n",
    "print(\"DYNAMIC MULTI-STATE DATA PROCESSING\")\n",
    "\n",
    "print(f\"\\nLoading data from: {CONFIG['input_file']}\")\n",
    "df = pd.read_csv(CONFIG['input_file'])\n",
    "\n",
    "print(f\"\\nOriginal data shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head(5))\n",
    "\n",
    "# ============================================\n",
    "# AUTO-DETECT STATES\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"AUTO-DETECTING STATES\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Get unique states from Province_State column\n",
    "states = sorted(df[CONFIG['state_column']].unique())\n",
    "\n",
    "print(f\"\\n AUTO-DETECTED STATES: {states}\")\n",
    "print(f\"   Found {len(states)} states in dataset\")\n",
    "\n",
    "# Count observations per state\n",
    "print(\"\\nObservations per state:\")\n",
    "for state in states:\n",
    "    count = len(df[df[CONFIG['state_column']] == state])\n",
    "    print(f\"   {state}: {count} weeks\")\n",
    "\n",
    "# ============================================\n",
    "# CHECK FOR DUPLICATES\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CHECKING FOR DUPLICATES\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Check if there are duplicate date-state combinations\n",
    "duplicates = df.groupby([CONFIG['date_column'], CONFIG['state_column']]).size()\n",
    "max_duplicates = duplicates.max()\n",
    "\n",
    "if max_duplicates > 1:\n",
    "    print(f\"\\n WARNING: Found {max_duplicates} duplicate rows per date-state combination!\")\n",
    "    print(\"Removing duplicates (keeping first occurrence)...\")\n",
    "    \n",
    "    df = df.drop_duplicates(subset=[CONFIG['date_column'], CONFIG['state_column']], keep='first')\n",
    "    print(f\" Removed duplicates. New shape: {df.shape}\")\n",
    "else:\n",
    "    print(\" No duplicates found\")\n",
    "\n",
    "# ============================================\n",
    "# PROCESS DATES\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PROCESSING DATES\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "df[CONFIG['date_column']] = pd.to_datetime(df[CONFIG['date_column']])\n",
    "df = df.sort_values([CONFIG['state_column'], CONFIG['date_column']]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Date range: {df[CONFIG['date_column']].min()} to {df[CONFIG['date_column']].max()}\")\n",
    "\n",
    "# Get unique dates\n",
    "unique_dates = sorted(df[CONFIG['date_column']].unique())\n",
    "print(f\"Unique dates: {len(unique_dates)}\")\n",
    "\n",
    "# Verify each state has same dates\n",
    "print(\"\\nVerifying date consistency across states:\")\n",
    "for state in states:\n",
    "    state_dates = len(df[df[CONFIG['state_column']] == state])\n",
    "    print(f\"  {state}: {state_dates} observations\")\n",
    "    \n",
    "    if state_dates != len(unique_dates):\n",
    "        print(f\"WARNING: {state} has different number of dates!\")\n",
    "\n",
    "# ============================================\n",
    "# CONVERT CUMULATIVE TO WEEKLY NEW\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONVERTING CUMULATIVE TO WEEKLY NEW CASES/DEATHS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Process each state separately (important for diff())\n",
    "df_processed_list = []\n",
    "\n",
    "for state in states:\n",
    "    print(f\"\\nProcessing {state}...\")\n",
    "    \n",
    "    # Get data for this state\n",
    "    state_df = df[df[CONFIG['state_column']] == state].copy()\n",
    "    state_df = state_df.sort_values(CONFIG['date_column']).reset_index(drop=True)\n",
    "    \n",
    "    # Convert cumulative to weekly NEW\n",
    "    if CONFIG['confirmed_column'] in state_df.columns:\n",
    "        state_df['new_cases'] = state_df[CONFIG['confirmed_column']].diff()\n",
    "        state_df.loc[0, 'new_cases'] = state_df.loc[0, CONFIG['confirmed_column']]  # First week\n",
    "        state_df['new_cases'] = state_df['new_cases'].clip(lower=0)\n",
    "        print(f\"  Created new_cases from {CONFIG['confirmed_column']}\")\n",
    "    \n",
    "    if CONFIG['deaths_column'] in state_df.columns:\n",
    "        state_df['new_deaths'] = state_df[CONFIG['deaths_column']].diff()\n",
    "        state_df.loc[0, 'new_deaths'] = state_df.loc[0, CONFIG['deaths_column']]  # First week\n",
    "        state_df['new_deaths'] = state_df['new_deaths'].clip(lower=0)\n",
    "        print(f\"  Created new_deaths from {CONFIG['deaths_column']}\")\n",
    "    \n",
    "    # Check for claims column\n",
    "    if CONFIG['claims_column'] not in state_df.columns:\n",
    "        print(f\"WARNING: {CONFIG['claims_column']} not found!\")\n",
    "    \n",
    "    df_processed_list.append(state_df)\n",
    "\n",
    "# Combine all states back together\n",
    "df_long = pd.concat(df_processed_list, ignore_index=True)\n",
    "df_long = df_long.sort_values([CONFIG['state_column'], CONFIG['date_column']]).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n Processed all states\")\n",
    "print(f\"Total observations: {len(df_long)} ({len(df_long)//len(states)} weeks × {len(states)} states)\")\n",
    "\n",
    "# ============================================\n",
    "# CREATE CLEAN LONG FORMAT\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CREATING CLEAN LONG FORMAT\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Select and rename columns\n",
    "df_long_clean = pd.DataFrame({\n",
    "    'date': df_long[CONFIG['date_column']],\n",
    "    'state': df_long[CONFIG['state_column']],\n",
    "    'claims': df_long[CONFIG['claims_column']],\n",
    "    'cases': df_long['new_cases'],\n",
    "    'deaths': df_long['new_deaths']\n",
    "})\n",
    "\n",
    "print(f\"\\nClean long format shape: {df_long_clean.shape}\")\n",
    "print(f\"Expected: {len(unique_dates) * len(states)} rows\")\n",
    "\n",
    "print(\"\\nFirst 9 rows (3 weeks × 3 states):\")\n",
    "print(df_long_clean.head(6))\n",
    "\n",
    "# Verify no duplicates in clean data\n",
    "dup_check = df_long_clean.groupby(['date', 'state']).size()\n",
    "if dup_check.max() > 1:\n",
    "    print(\"\\n ERROR: Still have duplicates in clean data!\")\n",
    "    print(\"Removing duplicates...\")\n",
    "    df_long_clean = df_long_clean.drop_duplicates(subset=['date', 'state'], keep='first')\n",
    "else:\n",
    "    print(\"\\n No duplicates in clean long format\")\n",
    "\n",
    "# ============================================\n",
    "# PIVOT TO WIDE FORMAT\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONVERTING TO WIDE FORMAT\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Pivot each metric separately\n",
    "print(\"\\nPivoting claims...\")\n",
    "claims_wide = df_long_clean.pivot(index='date', columns='state', values='claims')\n",
    "claims_wide.columns = [f'{col}_claims' for col in claims_wide.columns]\n",
    "\n",
    "print(\"Pivoting cases...\")\n",
    "cases_wide = df_long_clean.pivot(index='date', columns='state', values='cases')\n",
    "cases_wide.columns = [f'{col}_cases' for col in cases_wide.columns]\n",
    "\n",
    "print(\"Pivoting deaths...\")\n",
    "deaths_wide = df_long_clean.pivot(index='date', columns='state', values='deaths')\n",
    "deaths_wide.columns = [f'{col}_deaths' for col in deaths_wide.columns]\n",
    "\n",
    "# Combine all pivoted dataframes\n",
    "df_wide = pd.concat([claims_wide, cases_wide, deaths_wide], axis=1)\n",
    "df_wide = df_wide.reset_index()\n",
    "\n",
    "# Reorder columns for readability (date, then all FL, then all GA, then all NY)\n",
    "ordered_cols = ['date']\n",
    "for state in sorted(states):\n",
    "    ordered_cols.extend([f'{state}_claims', f'{state}_cases', f'{state}_deaths'])\n",
    "\n",
    "df_wide = df_wide[ordered_cols]\n",
    "\n",
    "print(f\"\\nWide format shape: {df_wide.shape}\")\n",
    "print(f\"Expected: {len(unique_dates)} rows × {1 + len(states)*3} columns\")\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_wide.head())\n",
    "\n",
    "# Verify no duplicate dates\n",
    "if df_wide['date'].duplicated().any():\n",
    "    print(\"\\n ERROR: Duplicate dates in wide format!\")\n",
    "    print(\"Number of duplicates:\", df_wide['date'].duplicated().sum())\n",
    "    print(\"\\nRemoving duplicate dates (keeping first)...\")\n",
    "    df_wide = df_wide.drop_duplicates(subset=['date'], keep='first')\n",
    "    print(f\" New shape: {df_wide.shape}\")\n",
    "else:\n",
    "    print(\"\\n No duplicate dates in wide format\")\n",
    "\n",
    "# ============================================\n",
    "# DATA QUALITY CHECKS\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in wide format:\")\n",
    "missing = df_wide.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"No missing values\")\n",
    "\n",
    "# Check for negative values\n",
    "print(\"\\nChecking for negative values:\")\n",
    "has_negatives = False\n",
    "for col in df_wide.columns:\n",
    "    if col != 'date':\n",
    "        neg_count = (df_wide[col] < 0).sum()\n",
    "        if neg_count > 0:\n",
    "            print(f\" {col}: {neg_count} negative values\")\n",
    "            has_negatives = True\n",
    "\n",
    "if not has_negatives:\n",
    "    print(\"  No negative values detected\")    \n",
    "\n",
    "# ============================================\n",
    "# TRAIN/TEST SPLIT\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TRAIN/TEST SPLIT - Last {CONFIG['test_weeks']} Weeks as Test\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "total_weeks = len(df_wide)\n",
    "test_size = CONFIG['test_weeks']\n",
    "train_size = total_weeks - test_size\n",
    "\n",
    "print(f\"\\nTotal weeks: {total_weeks}\")\n",
    "print(f\"Train size: {train_size} weeks ({train_size/total_weeks*100:.1f}%)\")\n",
    "print(f\"Test size: {test_size} weeks ({test_size/total_weeks*100:.1f}%)\")\n",
    "\n",
    "# Split wide format\n",
    "train_data_wide = df_wide.iloc[:train_size].copy()\n",
    "test_data_wide = df_wide.iloc[train_size:].copy()\n",
    "\n",
    "# Split long format\n",
    "train_dates = train_data_wide['date'].tolist()\n",
    "test_dates = test_data_wide['date'].tolist()\n",
    "\n",
    "train_data_long = df_long_clean[df_long_clean['date'].isin(train_dates)].copy()\n",
    "test_data_long = df_long_clean[df_long_clean['date'].isin(test_dates)].copy()\n",
    "\n",
    "# Verify splits\n",
    "print(f\"\\nWIDE FORMAT:\")\n",
    "print(f\"  Train: {len(train_data_wide)} weeks\")\n",
    "print(f\"  Test: {len(test_data_wide)} weeks\")\n",
    "\n",
    "print(f\"\\nLONG FORMAT:\")\n",
    "print(f\"  Train: {len(train_data_long)} rows ({len(train_data_long)//len(states)} weeks × {len(states)} states)\")\n",
    "print(f\"  Test: {len(test_data_long)} rows ({len(test_data_long)//len(states)} weeks × {len(states)} states)\")\n",
    "\n",
    "print(f\"\\nDate ranges:\")\n",
    "print(f\"  Train: {train_data_wide['date'].min()} to {train_data_wide['date'].max()}\")\n",
    "print(f\"  Test: {test_data_wide['date'].min()} to {test_data_wide['date'].max()}\")\n",
    "\n",
    "# ============================================\n",
    "# SAVE PROCESSED DATA\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SAVING PROCESSED DATA\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Wide format\n",
    "df_wide.to_csv(CONFIG['output_wide'], index=False)\n",
    "print(f\" {CONFIG['output_wide']}\")\n",
    "\n",
    "train_data_wide.to_csv(CONFIG['output_train_wide'], index=False)\n",
    "print(f\" {CONFIG['output_train_wide']}\")\n",
    "\n",
    "test_data_wide.to_csv(CONFIG['output_test_wide'], index=False)\n",
    "print(f\" {CONFIG['output_test_wide']}\")\n",
    "\n",
    "# Long format\n",
    "train_data_long.to_csv(CONFIG['output_train_long'], index=False)\n",
    "print(f\" {CONFIG['output_train_long']}\")\n",
    "\n",
    "test_data_long.to_csv(CONFIG['output_test_long'], index=False)\n",
    "print(f\" {CONFIG['output_test_long']}\")\n",
    "\n",
    "# States list\n",
    "with open('states_list.txt', 'w') as f:\n",
    "    f.write(','.join(states))\n",
    "print(f\" states_list.txt\")\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZATION\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CREATING VISUALIZATION\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "n_states = len(states)\n",
    "fig, axes = plt.subplots(n_states, 2, figsize=(16, 5*n_states))\n",
    "\n",
    "if n_states == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for idx, state in enumerate(states):\n",
    "    # Filter data for this state\n",
    "    state_full = df_long_clean[df_long_clean['state'] == state]\n",
    "    state_train = train_data_long[train_data_long['state'] == state]\n",
    "    state_test = test_data_long[test_data_long['state'] == state]\n",
    "    \n",
    "    # Plot claims\n",
    "    ax = axes[idx, 0]\n",
    "    ax.plot(state_full['date'], state_full['claims'], \n",
    "            color='lightgray', alpha=0.5, linewidth=1)\n",
    "    ax.plot(state_train['date'], state_train['claims'],\n",
    "            color='blue', linewidth=2.5, label=f'Train ({len(state_train)} weeks)', \n",
    "            marker='o', markersize=3)\n",
    "    ax.plot(state_test['date'], state_test['claims'],\n",
    "            color='red', linewidth=2.5, label=f'Test ({len(state_test)} weeks)', \n",
    "            marker='s', markersize=4)\n",
    "    \n",
    "    split_date = train_data_wide['date'].max()\n",
    "    ax.axvline(split_date, color='black', linestyle='--', linewidth=2)\n",
    "\n",
    "    ax.set_title(f'{state} - Unemployment Claims', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Claims (thousands)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot cases\n",
    "    ax2 = axes[idx, 1]\n",
    "    ax2.plot(state_full['date'], state_full['cases'], \n",
    "             color='lightgray', alpha=0.5, linewidth=1)\n",
    "    ax2.plot(state_train['date'], state_train['cases'],\n",
    "             color='blue', linewidth=2.5, label=f'Train ({len(state_train)} weeks)', \n",
    "             marker='o', markersize=3)\n",
    "    ax2.plot(state_test['date'], state_test['cases'],\n",
    "             color='red', linewidth=2.5, label=f'Test ({len(state_test)} weeks)', \n",
    "             marker='s', markersize=4)\n",
    "    \n",
    "    ax2.axvline(split_date, color='black', linestyle='--', linewidth=2)\n",
    "    \n",
    "    ax2.set_title(f'{state} - COVID Cases', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Weekly New Cases')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['output_viz'], dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved {CONFIG['output_viz']}\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\n Successfully processed {len(states)} states: {', '.join(states)}\")\n",
    "print(f\" Wide format: {len(df_wide)} weeks (NO DUPLICATES)\")\n",
    "print(f\" Long format: {len(df_long_clean)} observations\")\n",
    "print(f\" Train: {train_size} weeks, Test: {test_size} weeks\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
