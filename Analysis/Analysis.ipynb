{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a194eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES\n",
    "\n",
    "# IMPORT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import us\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import Autoformer\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "import requests\n",
    "import certifi\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import itertools\n",
    "\n",
    "import math\n",
    "from typing import List, Optional, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "parent = os.path.dirname(os.getcwd())\n",
    "data_path = os.path.join(parent, \"data\")\n",
    "\n",
    "# Add to path if not already present\n",
    "if data_path not in sys.path:\n",
    "    sys.path.append(data_path)\n",
    "\n",
    "from data.GetCovidData import getData\n",
    "from data.FredAPIKey import fred\n",
    "\n",
    "\n",
    "from fredapi import Fred\n",
    "import pandas as pd\n",
    "from fredapi import Fred\n",
    "import pandas as pd\n",
    "#fred = Fred(api_key='API_KEY_HERE')\n",
    "\n",
    "#fred = Fred(api_key='f9a44139decd5e780297cade865dd2eb')\n",
    "# TL's key: f9a44139decd5e780297cade865dd2eb\n",
    "\n",
    "\n",
    "#from FredAPIKey import fred\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca732684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COVID-19 Data (daily reports)\n",
    "\n",
    "def getData(column='Confirmed', startDate=\"04-12-2020\", endDate=\"03-09-2023\"):\n",
    "    try:\n",
    "        minDate = datetime.strptime(\"04-12-2020\", \"%m-%d-%Y\")\n",
    "        maxDate = datetime.strptime(\"03-09-2023\", \"%m-%d-%Y\")\n",
    "        startDateTime = max(datetime.strptime(startDate, \"%m-%d-%Y\"), minDate)\n",
    "        endDateTime = min(datetime.strptime(endDate, \"%m-%d-%Y\"), maxDate)\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing dates:\", e)\n",
    "        return None\n",
    "\n",
    "    all_data = []\n",
    "    all_dates = []\n",
    "    provinces_union = set()\n",
    "\n",
    "    #Load all CSVs and collect data\n",
    "    for offset in range((endDateTime - startDateTime).days + 1):\n",
    "        day = startDateTime + timedelta(days=offset)\n",
    "        url = (\n",
    "            \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/\"\n",
    "            \"csse_covid_19_data/csse_covid_19_daily_reports_us/\"\n",
    "            + day.strftime(\"%m-%d-%Y\") + \".csv\"\n",
    "        )\n",
    "        try:\n",
    "            \n",
    "            #column=['Deaths', 'Recovered', 'Active']\n",
    "            df = pd.read_csv(url)\n",
    "            if column not in df.columns:\n",
    "                print(f\"{day.strftime('%m-%d-%Y')} missing '{column}' column.\")\n",
    "                continue\n",
    "            df = df[['Province_State', column]].rename(columns={column: day.strftime(\"%m-%d-%Y\")})\n",
    "            all_data.append(df)\n",
    "            all_dates.append(day.strftime(\"%m-%d-%Y\"))\n",
    "            provinces_union.update(df['Province_State'])\n",
    "\n",
    "            \"\"\"\n",
    "            requested_cols = ['Deaths', 'Recovered', 'Active']\n",
    "            df = pd.read_csv(url)\n",
    "\n",
    "            # Filter to existing requested columns\n",
    "            existing_cols = [c for c in requested_cols if c in df.columns]\n",
    "            \n",
    "            if not existing_cols:\n",
    "                print(f\"{day.strftime('%m-%d-%Y')} missing all requested columns: {requested_cols}\")\n",
    "                continue\n",
    "\n",
    "            # Select Province_State plus existing columns\n",
    "            df = df[['Province_State'] + existing_cols]\n",
    "\n",
    "            # Rename: append date suffix to each metric column\n",
    "            rename_map = {\n",
    "                c: f\"{c}_{day.strftime('%m-%d-%Y')}\" for c in existing_cols\n",
    "            }\n",
    "            df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "            # Append results\n",
    "            all_data.append(df)\n",
    "            all_dates.append(day.strftime(\"%m-%d-%Y\"))\n",
    "            provinces_union.update(df['Province_State'])\n",
    "            \"\"\"\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {day.strftime('%m-%d-%Y')} — {type(e).__name__}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"No data loaded.\")\n",
    "        return None\n",
    "\n",
    "    #Merge all dataframes by Province_State\n",
    "    full_df = pd.DataFrame({'Province_State': sorted(provinces_union)})\n",
    "    for df in all_data:\n",
    "        full_df = full_df.merge(df, on='Province_State', how='left')\n",
    "\n",
    "    #Convert columns to numeric\n",
    "    for col in full_df.columns[1:]:\n",
    "        full_df[col] = pd.to_numeric(full_df[col], errors='coerce')\n",
    "\n",
    "    print(f\"Successfully loaded {len(all_data)} file(s) covering {all_dates[0]} → {all_dates[-1]}\")\n",
    "    return full_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f29dc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# COVID-19 U.S. Daily Reports — Data Documentation\n",
    "# Source (JHU CSSE):\n",
    "# https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports_us/04-30-2020.csv\n",
    "# https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports_us/03-11-2021.csv\n",
    "#\n",
    "# Columns included (per state):\n",
    "# Province_State, Country_Region, Last_Update, Lat, Long_,\n",
    "# Confirmed, Deaths, Recovered, Active, FIPS, Incident_Rate,\n",
    "# Total_Test_Results, People_Hospitalized, Case_Fatality_Ratio,\n",
    "# UID, ISO3, Testing_Rate, Hospitalization_Rate, Date,\n",
    "# People_Tested, Mortality_Rate\n",
    "#\n",
    "# Subset used here:\n",
    "# Province (state), Confirmed on 04-30-2020, Confirmed on 03-11-2021\n",
    "\n",
    "\n",
    "\n",
    "df = getData(column='Deaths')\n",
    "\n",
    "df.columns = ['Province_State'] + list(pd.to_datetime(df.columns[1:], format=\"%m-%d-%Y\"))\n",
    "\n",
    "df = df.set_index('Province_State')\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "newYork = df.loc['New York'] \n",
    "\n",
    "\n",
    "df_sum = (\n",
    "    df.drop(columns=['Province_State'], errors='ignore')\n",
    "      .sum()\n",
    "      .reset_index()\n",
    "      .rename(columns={'index': 'Date', 0: 'Total_Confirmed'})\n",
    ")\n",
    "\n",
    "df_mean = (\n",
    "    df.drop(columns=['Province_State', 'Province'], errors='ignore')\n",
    "      .select_dtypes(include='number')                  \n",
    "      .mean()                                           \n",
    "      .reset_index()\n",
    "      .rename(columns={'index': 'Date', 0: 'Average_Confirmed'})\n",
    ")\n",
    "\n",
    "print(df_mean.head())\n",
    "\n",
    "url = \"https://data.cdc.gov/resource/pwn4-m3yp.csv\"\n",
    "\n",
    "response = requests.get(url, verify=certifi.where())\n",
    "response.raise_for_status()\n",
    "\n",
    "covid_cdc = pd.read_csv(StringIO(response.text))\n",
    "\n",
    "print(\"Loaded:\", covid_cdc.shape)\n",
    "print(covid_cdc.head())\n",
    "\n",
    "\n",
    "\n",
    "first_col = covid_cdc.columns[0]\n",
    "print(\"Aggregating by:\", first_col)\n",
    "\n",
    "byState = covid_cdc.groupby(first_col).sum(numeric_only=True)\n",
    "\n",
    "byState.columns\n",
    "byState.index = pd.to_datetime(byState.index)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(\n",
    "    byState.index,\n",
    "    byState['new_deaths'],\n",
    "    color='orange',\n",
    "    linewidth=2,\n",
    "    marker='o',\n",
    "    label='COVID-19: Aggregated USA Weekly New Deaths'\n",
    ")\n",
    "\n",
    "plt.title(\"COVID-19: Aggregated USA Weekly New Deaths\", fontsize=14)\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Number of Deaths\", fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.gca().xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(\n",
    "    byState.index,\n",
    "    byState['new_cases'],\n",
    "    color='orange',\n",
    "    linewidth=2,\n",
    "    marker='o',\n",
    "    label='COVID-19: Aggregated USA Weekly New Cases'\n",
    ")\n",
    "\n",
    "plt.title(\"COVID-19: Aggregated USA Weekly New Cases\", fontsize=14)\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Number of Cases\", fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.gca().xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc47e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full import OF DATASET\n",
    "\n",
    "\n",
    "def fix_date(date_str):\n",
    "    try:\n",
    "        # parse MM-DD-YYYY\n",
    "        d = dt.datetime.strptime(date_str, \"%m-%d-%Y\")\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Invalid date format: {date_str}. Expected MM-DD-YYYY.\")\n",
    "    \n",
    "    # Return in FRED format\n",
    "    return d.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    \n",
    "def getCombinedData(state='NY', column='Confirmed', startDate=\"04-12-2020\", endDate=\"03-09-2023\"):\n",
    "    # Convert a state abbreviation to the full state name (for example, \"NY\" to \"New York\")\n",
    "    full_name = us.states.lookup(state)\n",
    "    if full_name is None:\n",
    "        raise ValueError(f\"Invalid state abbreviation: {state}\")\n",
    "    full_name = full_name.name\n",
    "\n",
    "    # Build the FRED series ID for unemployment claims (for example, \"NYICLAIMS\")\n",
    "    series_id = f'{state}ICLAIMS'\n",
    "\n",
    "    # Convert dates from MM-DD-YYYY to the required FRED format YYYY-MM-DD\n",
    "    observation_start = fix_date(startDate)\n",
    "    observation_end   = fix_date(endDate)\n",
    "\n",
    "    # Retrieve the unemployment claims time series from FRED\n",
    "    claim = fred.get_series(series_id,\n",
    "                            observation_start=observation_start,\n",
    "                            observation_end=observation_end)\n",
    "\n",
    "    # Retrieve the COVID time series for all states\n",
    "    df = getData(column, startDate, endDate)\n",
    "\n",
    "    # Convert all date columns to datetime objects and set Province_State as the index\n",
    "    df.columns = ['Province_State'] + list(pd.to_datetime(df.columns[1:], format=\"%m-%d-%Y\"))\n",
    "    df = df.set_index('Province_State')\n",
    "\n",
    "    # Extract the COVID time series for the selected state\n",
    "    congruentDF = df.loc[full_name]\n",
    "\n",
    "    # Combine the unemployment claims and COVID series using only the dates that appear in both\n",
    "    joined = pd.concat([claim, congruentDF], axis=1, join='inner')\n",
    "    joined.columns = ['claims', column]  # Rename columns for clarity\n",
    "\n",
    "    return joined\n",
    "\n",
    "def build_state_combined_data(state, extra_columns=['Deaths', 'Recovered', 'Active']):\n",
    "   \n",
    "\n",
    "    # Base data \n",
    "    final_df = getCombinedData(state=state).set_index('claims', append=True)\n",
    "\n",
    "    # Fetch each extra metric one by one and join\n",
    "    for col in extra_columns:\n",
    "        try:\n",
    "            temp_df = getCombinedData(state=state, column=col).set_index('claims', append=True)\n",
    "            final_df = final_df.join(temp_df, how='inner')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: {col} missing for {state} — SKIPPED ({type(e).__name__})\")\n",
    "\n",
    "    # Reset index and rename first two columns\n",
    "    final_df = final_df.reset_index()\n",
    "    final_df.rename(columns={final_df.columns[0]: 'Date', final_df.columns[1]: 'claims'}, inplace=True)\n",
    "\n",
    "   \n",
    "    final_df.set_index('Date', inplace=True)\n",
    "    # Add state name\n",
    "    final_df['Province_State'] = state\n",
    "\n",
    "    # Weekly new cases & deaths\n",
    "    final_df['New_Weekly_Cases'] = final_df['Confirmed'].diff().fillna(0)\n",
    "    final_df['New_Weekly_Deaths'] = final_df['Deaths'].diff().fillna(0)\n",
    "\n",
    "    # Reorder columns (optional)\n",
    "    cols = ['Date', 'Province_State', 'claims', 'Confirmed', 'Deaths', \n",
    "        'New_Weekly_Cases', 'New_Weekly_Deaths', 'Recovered', 'Active']\n",
    "    # Only keep columns that exist (Recovered/Active may be missing)\n",
    "    final_df = final_df[[col for col in cols if col in final_df.columns]]\n",
    "\n",
    "    # Save to CSV\n",
    "    output_name = f\"Final_{state}_CombinedData.csv\"\n",
    "    final_df.to_csv(output_name, index=True)\n",
    "\n",
    "    print(f\"Saved combined data → {output_name}\")\n",
    "    return final_df\n",
    "\n",
    "\n",
    "\n",
    "def build_multiple_states(states, extra_columns=['Deaths', 'Recovered', 'Active']):\n",
    "    \"\"\" Accepts a single state string or a list of state abbreviations. \"\"\"\n",
    "    \n",
    "    # Normalize input to always be a list\n",
    "    if isinstance(states, str):\n",
    "        states = [states]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for st in states:\n",
    "        print(f\"\\nProcessing: {st}\")\n",
    "        results[st] = build_state_combined_data(st, extra_columns)\n",
    "\n",
    "    print(\"\\n✔ Done with all states!\")\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose the states \n",
    "    states_to_build = ['FL','GA','NY','CA','TX','WA','IL']\n",
    "\n",
    "    # Build multiple states at once\n",
    "    results = build_multiple_states(states_to_build)\n",
    "\n",
    "    print(\"All states completed! Files saved:\")\n",
    "    for state in results:\n",
    "        print(f\"  Final_{state}_CombinedData.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc776a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "fred = Fred(api_key='f9a44139decd5e780297cade865dd2eb')\n",
    "\n",
    "\n",
    "\n",
    "state_mapping = {\n",
    "    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas',\n",
    "    'CA': 'California', 'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware',\n",
    "    'FL': 'Florida', 'GA': 'Georgia', 'HI': 'Hawaii', 'ID': 'Idaho',\n",
    "    'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas',\n",
    "    'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n",
    "    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi',\n",
    "    'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada',\n",
    "    'NH': 'New Hampshire', 'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY': 'New York',\n",
    "    'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma',\n",
    "    'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n",
    "    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah',\n",
    "    'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia',\n",
    "    'WI': 'Wisconsin', 'WY': 'Wyoming', 'PR': 'Puerto Rico', 'VI': 'Virgin Islands'\n",
    "}\n",
    "\n",
    "states = list(state_mapping.keys())\n",
    "\n",
    "# Download initial state-level claims (ICSA = Initial Claims by State)\n",
    "print('Downloading Initial Claims data...')\n",
    "state_claims = {}\n",
    "for state in states:\n",
    "    series_id = f'{state}ICLAIMS'\n",
    "    try:\n",
    "        state_claims[state] = fred.get_series(series_id,\n",
    "                                                observation_start='2020-02-28',\n",
    "                                                observation_end='2023-03-18')\n",
    "        print(f'Successfully retrieved data for {state}')\n",
    "    except:\n",
    "        print(f'Failed to retrieve data for {state}')\n",
    "\n",
    "# Combine all state data into a single DataFrame\n",
    "df_state_claims = pd.DataFrame(state_claims)\n",
    "df_state_claims.to_csv('state_unemployment_claims.csv')\n",
    "\n",
    "# Melt the DataFrame to long format\n",
    "df_melted = df_state_claims.reset_index().melt(id_vars='index', var_name='Province_State', value_name='Initial_Claims')\n",
    "\n",
    "df_melted.rename(columns={'index': 'Date'}, inplace=True)\n",
    "df_melted.to_csv('state_unemployment_claims_long.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcdd6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMAX BASE\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ARIMAX MODEL - SINGLE STATE WITH COVID FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load states list\n",
    "with open('states_list.txt', 'r') as f:\n",
    "    states = f.read().strip().split(',')\n",
    "\n",
    "print(f\"\\n Detected states: {states}\")\n",
    "\n",
    "# Load training and test data (wide format)\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "\n",
    "print(f\"\\nTrain data: {len(train_data)} weeks\")\n",
    "print(f\"Test data: {len(test_data)} weeks\")\n",
    "print(f\"Train date range: {train_data['date'].min()} to {train_data['date'].max()}\")\n",
    "print(f\"Test date range: {test_data['date'].min()} to {test_data['date'].max()}\")\n",
    "\n",
    "# ============================================\n",
    "# ARIMAX CONFIGURATION - GRID SEARCH with CV\n",
    "# ============================================\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_arimax_model(y, exog, order, seasonal_order):\n",
    "    \"\"\"Evaluate ARIMAX model using time series cross-validation\"\"\"\n",
    "    try:\n",
    "        model = SARIMAX(y, exog=exog, order=order, \n",
    "                       seasonal_order=seasonal_order,\n",
    "                       enforce_stationarity=False,\n",
    "                       enforce_invertibility=False)\n",
    "        fitted = model.fit(disp=False, maxiter=100)\n",
    "        return fitted.aic, fitted.bic\n",
    "    except:\n",
    "        return np.inf, np.inf\n",
    "\n",
    "# Grid search\n",
    "p_range = range(0, 3)\n",
    "d_range = range(0, 2)\n",
    "q_range = range(0, 3)\n",
    "P_range = range(0, 2)\n",
    "D_range = range(0, 2)\n",
    "Q_range = range(0, 2)\n",
    "s = 52\n",
    "\n",
    "best_aic = np.inf\n",
    "best_order = None\n",
    "best_seasonal = None\n",
    "\n",
    "print(\"Grid searching for optimal order...\")\n",
    "\n",
    "for state in states:\n",
    "    y = train_data[f'{state}_claims']\n",
    "    X = train_data[[f'{state}_cases', f'{state}_deaths']]\n",
    "    \n",
    "    print(f\"\\nSearching for {state}...\")\n",
    "    \n",
    "    \n",
    "    orders_to_try = [\n",
    "        ((1,1,1), (0,0,0,0)),  # Simple, no seasonal\n",
    "        ((1,1,1), (1,0,1,52)), # Simple with seasonal\n",
    "        ((2,1,2), (0,0,0,0)),  # Medium, no seasonal\n",
    "        ((2,1,2), (1,0,1,52)), \n",
    "        ((3,1,3), (1,0,1,52)), # Complex\n",
    "        ((1,1,0), (0,0,0,0)),  # AR only\n",
    "        ((0,1,1), (0,0,0,0)),  # MA only\n",
    "        ((2,1,0), (1,0,0,52)), # AR with seasonal AR\n",
    "    ]\n",
    "    \n",
    "    for order, seasonal_order in orders_to_try:\n",
    "        aic, bic = evaluate_arimax_model(y, X, order, seasonal_order)\n",
    "        print(f\"  {order} {seasonal_order}: AIC={aic:.0f}, BIC={bic:.0f}\")\n",
    "        \n",
    "        if aic < best_aic:\n",
    "            best_aic = aic\n",
    "            best_order = order\n",
    "            best_seasonal = seasonal_order\n",
    "    \n",
    "    print(f\"\\n Best for {state}: {best_order} {best_seasonal} (AIC: {best_aic:.0f})\")\n",
    "\n",
    "ARIMAX_CONFIG = {\n",
    "    'order': best_order,\n",
    "    'seasonal_order': best_seasonal,\n",
    "    'enforce_stationarity': False,\n",
    "    'enforce_invertibility': False,\n",
    "    'maxiter': 200,\n",
    "    'disp': False\n",
    "}\n",
    "\n",
    "print(f\"\\nARIMAX Configuration:\")\n",
    "print(f\"  Order (p,d,q): {ARIMAX_CONFIG['order']}\")\n",
    "print(f\"  Seasonal Order (P,D,Q,s): {ARIMAX_CONFIG['seasonal_order']}\")\n",
    "\n",
    "# ============================================\n",
    "# FIT ARIMAX FOR EACH STATE\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FITTING ARIMAX MODELS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "arimax_results = {}\n",
    "\n",
    "for state in states:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"STATE: {state}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Extract target variable (claims)\n",
    "    y_train = train_data[f'{state}_claims'].copy()\n",
    "    y_test = test_data[f'{state}_claims'].copy()\n",
    "    \n",
    "    # Extract exogenous variables (COVID features)\n",
    "    X_train = train_data[[f'{state}_cases', f'{state}_deaths']].copy()\n",
    "    X_test = test_data[[f'{state}_cases', f'{state}_deaths']].copy()\n",
    "    \n",
    "    print(f\"\\nTarget variable: {state}_claims\")\n",
    "    print(f\"  Train size: {len(y_train)}\")\n",
    "    print(f\"  Test size: {len(y_test)}\")\n",
    "    \n",
    "    print(f\"\\nExogenous variables: {X_train.columns.tolist()}\")\n",
    "    print(f\"  Train shape: {X_train.shape}\")\n",
    "    print(f\"  Test shape: {X_test.shape}\")\n",
    "    \n",
    "    # FIXED: Check for missing values correctly (both lines)\n",
    "    train_missing = y_train.isnull().sum() + X_train.isnull().sum().sum()\n",
    "    if train_missing > 0:\n",
    "        print(f\"\\n WARNING: {train_missing} missing values in training data\")\n",
    "        print(f\"  y_train missing: {y_train.isnull().sum()}\")\n",
    "        print(f\"  X_train missing:\\n{X_train.isnull().sum()}\")\n",
    "        print(\"Filling with forward fill...\")\n",
    "        y_train = y_train.fillna(method='ffill')\n",
    "        X_train = X_train.fillna(method='ffill')\n",
    "    \n",
    "    test_missing = y_test.isnull().sum() + X_test.isnull().sum().sum()\n",
    "    if test_missing > 0:\n",
    "        print(f\"\\n WARNING: {test_missing} missing values in test data\")\n",
    "        print(f\"  y_test missing: {y_test.isnull().sum()}\")\n",
    "        print(f\"  X_test missing:\\n{X_test.isnull().sum()}\")\n",
    "        print(\"Filling with forward fill...\")\n",
    "        y_test = y_test.fillna(method='ffill')\n",
    "        X_test = X_test.fillna(method='ffill')\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nFitting ARIMAX model...\")\n",
    "            \n",
    "        model = SARIMAX(\n",
    "            y_train,\n",
    "            exog=X_train,\n",
    "            order=ARIMAX_CONFIG['order'],\n",
    "            seasonal_order=ARIMAX_CONFIG['seasonal_order'],\n",
    "            enforce_stationarity=ARIMAX_CONFIG['enforce_stationarity'],\n",
    "            enforce_invertibility=ARIMAX_CONFIG['enforce_invertibility']\n",
    "        )\n",
    "        \n",
    "        fitted_model = model.fit(\n",
    "            disp=ARIMAX_CONFIG['disp'],\n",
    "            maxiter=ARIMAX_CONFIG['maxiter']\n",
    "        )\n",
    "        \n",
    "        print(f\" Model fitted successfully\")\n",
    "        \n",
    "        # Make predictions\n",
    "        print(f\"\\nGenerating forecasts...\")\n",
    "        forecast = fitted_model.forecast(steps=len(test_data), exog=X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_test, forecast)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, forecast))\n",
    "        \n",
    "        smape = np.mean(2 * np.abs(forecast - y_test) / (np.abs(y_test) + np.abs(forecast))) * 100\n",
    "        r2 = r2_score(y_test, forecast)\n",
    "        \n",
    "        # Direction accuracy\n",
    "        actual_direction = np.sign(np.diff(y_test.values))\n",
    "        pred_direction = np.sign(np.diff(forecast.values))\n",
    "        direction_acc = np.mean(actual_direction == pred_direction) * 100\n",
    "        \n",
    "        # Store results\n",
    "        arimax_results[state] = {\n",
    "            'model': fitted_model,\n",
    "            'forecast': forecast,\n",
    "            'actual': y_test,\n",
    "            'train_dates': train_data['date'],\n",
    "            'test_dates': test_data['date'],\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE': smape,\n",
    "            'R2': r2,\n",
    "            'Direction_Acc': direction_acc,\n",
    "            'AIC': fitted_model.aic,\n",
    "            'BIC': fitted_model.bic\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RESULTS FOR {state}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"  MAE:                {mae:,.0f}\")\n",
    "        print(f\"  RMSE:               {rmse:,.0f}\")\n",
    "        print(f\"  MAPE:               {smape:.2f}%\")\n",
    "        print(f\"  R²:                 {r2:.3f}\")\n",
    "        print(f\"  Direction Accuracy: {direction_acc:.1f}%\")\n",
    "        print(f\"  AIC:                {fitted_model.aic:.0f}\")\n",
    "        print(f\"  BIC:                {fitted_model.bic:.0f}\")\n",
    "        \n",
    "        print(f\"\\n  Converged: {fitted_model.mle_retvals['converged']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n ERROR: ARIMAX failed for {state}\")\n",
    "        print(f\"   {str(e)}\")\n",
    "        print(f\"\\n   Trying simpler model specification...\")\n",
    "        \n",
    "        try:\n",
    "            model_simple = SARIMAX(\n",
    "                y_train,\n",
    "                exog=X_train,\n",
    "                order=(1, 1, 1),\n",
    "                seasonal_order=(0, 0, 0, 0),\n",
    "                enforce_stationarity=False,\n",
    "                enforce_invertibility=False\n",
    "            )\n",
    "            \n",
    "            fitted_model = model_simple.fit(disp=False, maxiter=100)\n",
    "            forecast = fitted_model.forecast(steps=len(test_data), exog=X_test)\n",
    "            \n",
    "            mae = mean_absolute_error(y_test, forecast)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, forecast))\n",
    "            \n",
    "            # Replace MAPE calculation with SMAPE\n",
    "            smape = np.mean(2 * np.abs(forecast - y_test) / (np.abs(y_test  ) + np.abs(forecast))) * 100\n",
    "            r2 = r2_score(y_test, forecast)\n",
    "            \n",
    "            actual_direction = np.sign(np.diff(y_test.values))\n",
    "            pred_direction = np.sign(np.diff(forecast.values))\n",
    "            direction_acc = np.mean(actual_direction == pred_direction) * 100\n",
    "            \n",
    "            arimax_results[state] = {\n",
    "                'model': fitted_model,\n",
    "                'forecast': forecast,\n",
    "                'actual': y_test,\n",
    "                'train_dates': train_data['date'],\n",
    "                'test_dates': test_data['date'],\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'MAPE': smape,\n",
    "                'R2': r2,\n",
    "                'Direction_Acc': direction_acc,\n",
    "                'AIC': fitted_model.aic,\n",
    "                'BIC': fitted_model.bic,\n",
    "                'note': 'Simplified model (1,1,1) with no seasonality'\n",
    "            }\n",
    "            \n",
    "            print(f\" Simpler model fitted successfully\")\n",
    "            print(f\"  MAE: {mae:,.0f}, RMSE: {rmse:,.0f}, R²: {r2:.3f}\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"   Simpler model also failed: {str(e2)}\")\n",
    "            arimax_results[state] = None\n",
    "\n",
    "# ============================================\n",
    "# SAVE RESULTS\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SAVING RESULTS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for state in states:\n",
    "    if arimax_results.get(state):\n",
    "        result = arimax_results[state]\n",
    "        summary_data.append({\n",
    "            'State': state,\n",
    "            'Model': 'ARIMAX',\n",
    "            'MAE': result['MAE'],\n",
    "            'RMSE': result['RMSE'],\n",
    "            'MAPE': result['MAPE'],\n",
    "            'R2': result['R2'],\n",
    "            'Direction_Acc': result['Direction_Acc'],\n",
    "            'AIC': result['AIC'],\n",
    "            'BIC': result['BIC'],\n",
    "            'Note': result.get('note', 'Full model')\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ARIMAX SUMMARY - ALL STATES\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_df.to_csv('arimax_results.csv', index=False)\n",
    "print(f\"\\n Saved 'arimax_results.csv'\")\n",
    "\n",
    "# Save individual forecasts\n",
    "for state in states:\n",
    "    if arimax_results.get(state):\n",
    "        forecast_df = pd.DataFrame({\n",
    "            'date': test_data['date'],\n",
    "            'actual': arimax_results[state]['actual'].values,\n",
    "            'forecast': arimax_results[state]['forecast'].values,\n",
    "            'error': arimax_results[state]['actual'].values - arimax_results[state]['forecast'].values\n",
    "        })\n",
    "        forecast_df.to_csv(f'arimax_forecast_{state}.csv', index=False)\n",
    "        print(f\" Saved 'arimax_forecast_{state}.csv'\")\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZATION\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"VISUALIZATIONS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "n_states = len([s for s in states if arimax_results.get(s)])\n",
    "\n",
    "if n_states > 0:\n",
    "    fig, axes = plt.subplots(n_states, 2, figsize=(16, 5*n_states))\n",
    "\n",
    "    if n_states == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    plot_idx = 0\n",
    "\n",
    "    for state in states:\n",
    "        if not arimax_results.get(state):\n",
    "            continue\n",
    "        \n",
    "        result = arimax_results[state]\n",
    "        \n",
    "        # Left: Actual vs Forecast\n",
    "        ax1 = axes[plot_idx, 0]\n",
    "        \n",
    "        ax1.plot(result['train_dates'], train_data[f'{state}_claims'],\n",
    "                 color='gray', alpha=0.3, linewidth=1, label='Training Data')\n",
    "        \n",
    "        ax1.plot(result['test_dates'], result['actual'],\n",
    "                 color='lightblue', linewidth=2.5, marker='o', markersize=5,\n",
    "                 label='Actual', zorder=3)\n",
    "        \n",
    "        ax1.plot(result['test_dates'], result['forecast'],\n",
    "                 color='red', linewidth=2.5, marker='s', markersize=4,\n",
    "                 linestyle='--', label='ARIMAX Forecast', alpha=0.8, zorder=2)\n",
    "        \n",
    "        ax1.axvline(result['train_dates'].max(), color='blue', \n",
    "                    linestyle='--', linewidth=2, alpha=0.5, label='Train/Test Split')\n",
    "        \n",
    "        ax1.set_title(f'{state} - ARIMAX Forecast vs Actual\\n' + \n",
    "                      f'MAE: {result[\"MAE\"]:,.0f} | RMSE: {result[\"RMSE\"]:,.0f} | R²: {result[\"R2\"]:.3f}',\n",
    "                      fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Date', fontsize=12)\n",
    "        ax1.set_ylabel('Unemployment Claims (thousands)', fontsize=12)\n",
    "        ax1.legend(loc='best', fontsize=10)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Right: Errors\n",
    "        ax2 = axes[plot_idx, 1]\n",
    "        \n",
    "        errors = result['actual'].values - result['forecast'].values\n",
    "        \n",
    "        ax2.plot(result['test_dates'], errors, \n",
    "                 color='purple', linewidth=2, marker='o', markersize=4)\n",
    "        ax2.axhline(0, color='black', linestyle='-', linewidth=1)\n",
    "        ax2.axhline(errors.mean(), color='red', linestyle='--', \n",
    "                    linewidth=2, label=f'Mean Error: {errors.mean():,.0f}')\n",
    "        \n",
    "        std_error = errors.std()\n",
    "        ax2.fill_between(result['test_dates'], -std_error, std_error,\n",
    "                         alpha=0.2, color='gray', label=f'±1 Std: {std_error:,.0f}')\n",
    "        \n",
    "        ax2.set_title(f'{state} - Forecast Errors\\n' + \n",
    "                      f'Mean: {errors.mean():,.0f} | Std: {errors.std():,.0f}',\n",
    "                      fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Date', fontsize=12)\n",
    "        ax2.set_ylabel('Forecast Error', fontsize=12)\n",
    "        ax2.legend(loc='best', fontsize=10)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plot_idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('arimax_forecasts.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\" Saved 'arimax_forecasts.png'\")\n",
    "    plt.close()\n",
    "\n",
    "# ============================================\n",
    "# RESIDUAL DIAGNOSTICS\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESIDUAL DIAGNOSTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if n_states > 0:\n",
    "    fig, axes = plt.subplots(n_states, 2, figsize=(16, 5*n_states))\n",
    "\n",
    "    if n_states == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    plot_idx = 0\n",
    "\n",
    "    for state in states:\n",
    "        if not arimax_results.get(state):\n",
    "            continue\n",
    "        \n",
    "        result = arimax_results[state]\n",
    "        residuals = result['model'].resid\n",
    "        \n",
    "        # Histogram\n",
    "        ax1 = axes[plot_idx, 0]\n",
    "        ax1.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "        ax1.axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "        ax1.set_title(f'{state} - Residual Distribution', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Residuals')\n",
    "        ax1.set_ylabel('Frequency')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Q-Q plot\n",
    "        \n",
    "        ax2 = axes[plot_idx, 1]\n",
    "        stats.probplot(residuals, dist=\"norm\", plot=ax2)\n",
    "        ax2.set_title(f'{state} - Q-Q Plot', fontsize=14, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plot_idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('arimax_diagnostics.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\" Saved 'arimax_diagnostics.png'\")\n",
    "    plt.close()\n",
    "\n",
    "# ============================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ARIMAX MODELING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "successful_states = len([s for s in states if arimax_results.get(s)])\n",
    "print(f\"\\n Successfully modeled {successful_states} / {len(states)} states\")\n",
    "\n",
    "if successful_states > 0:\n",
    "    print(f\"\\n PERFORMANCE SUMMARY:\")\n",
    "    for state in states:\n",
    "        if arimax_results.get(state):\n",
    "            result = arimax_results[state]\n",
    "            print(f\"\\n  {state}:\")\n",
    "            print(f\"    MAE:  {result['MAE']:>10,.0f}\")\n",
    "            print(f\"    RMSE: {result['RMSE']:>10,.0f}\")\n",
    "            print(f\"    R²:   {result['R2']:>10.3f}\")\n",
    "            print(f\"    MAPE: {result['MAPE']:>10.2f}%\")\n",
    "\n",
    "    print(f\"\\n FILES CREATED:\")\n",
    "    print(f\"    arimax_results.csv\")\n",
    "    for state in states:\n",
    "        if arimax_results.get(state):\n",
    "            print(f\"    arimax_forecast_{state}.csv\")\n",
    "    print(f\"    arimax_forecasts.png\")\n",
    "    print(f\"    arimax_diagnostics.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b14259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMAX INTERVENTION\n",
    "\"\"\"\n",
    "ARIMAX with Intervention Analysis\n",
    "Purpose: Explicitly model pandemic policy interventions\n",
    "Methods: Add intervention dummy variables for:\n",
    "  - Initial shutdown\n",
    "  - Reopening\n",
    "  - Delta/Omicron waves\n",
    "  - Vaccine rollout\n",
    "\"\"\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('Results/intervention', exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ARIMAX WITH INTERVENTION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================\n",
    "# LOAD DATA\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nLoading data...\")\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "\n",
    "with open('states_list.txt', 'r') as f:\n",
    "    states = f.read().strip().split(',')\n",
    "\n",
    "print(f\"States: {states}\")\n",
    "print(f\"Train: {len(train_data)} weeks ({train_data['date'].min()} to {train_data['date'].max()})\")\n",
    "print(f\"Test: {len(test_data)} weeks ({test_data['date'].min()} to {test_data['date'].max()})\")\n",
    "\n",
    "# ============================================\n",
    "# DEFINE INTERVENTION PERIODS\n",
    "# ============================================\n",
    "\n",
    "# Key pandemic intervention periods\n",
    "interventions = {\n",
    "    'initial_shutdown': ('2020-03-15', '2020-05-31'),\n",
    "    'reopening': ('2020-06-01', '2020-08-31'),\n",
    "    'second_wave': ('2020-11-01', '2021-02-28'),\n",
    "    'vaccine_rollout': ('2021-01-01', '2021-06-30'),\n",
    "    'delta_wave': ('2021-07-01', '2021-10-31'),\n",
    "    'omicron_wave': ('2021-12-01', '2022-02-28')\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DEFINED INTERVENTION PERIODS\")\n",
    "print(f\"{'='*80}\")\n",
    "for name, (start, end) in interventions.items():\n",
    "    print(f\"  {name:20s}: {start} to {end}\")\n",
    "\n",
    "# ============================================\n",
    "# CREATE INTERVENTION DUMMIES\n",
    "# ============================================\n",
    "\n",
    "def create_intervention_dummies(dates, interventions):\n",
    "    \"\"\"Create dummy variables for each intervention period\"\"\"\n",
    "    \n",
    "    dummies = pd.DataFrame(index=dates.index)\n",
    "    \n",
    "    for name, (start, end) in interventions.items():\n",
    "        start_date = pd.to_datetime(start)\n",
    "        end_date = pd.to_datetime(end)\n",
    "        \n",
    "        dummies[name] = ((dates >= start_date) & (dates <= end_date)).astype(int)\n",
    "    \n",
    "    return dummies\n",
    "\n",
    "# Create intervention dummies for train and test\n",
    "train_interventions = create_intervention_dummies(train_data['date'], interventions)\n",
    "test_interventions = create_intervention_dummies(test_data['date'], interventions)\n",
    "\n",
    "print(f\"\\nCreated {len(interventions)} intervention dummy variables\")\n",
    "print(f\"\\nTrain intervention counts (weeks in each period):\")\n",
    "print(train_interventions.sum())\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZE INTERVENTIONS\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CREATING INTERVENTION VISUALIZATION\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "fig, axes = plt.subplots(len(states), 1, figsize=(16, 5*len(states)))\n",
    "\n",
    "if len(states) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, state in enumerate(states):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot claims\n",
    "    ax.plot(train_data['date'], train_data[f'{state}_claims'],\n",
    "            linewidth=2, label='Unemployment Claims', color='blue', zorder=5)\n",
    "    \n",
    "    # Shade intervention periods\n",
    "    colors = ['red', 'orange', 'yellow', 'green', 'cyan', 'purple']\n",
    "    for (name, (start, end)), color in zip(interventions.items(), colors):\n",
    "        start_date = pd.to_datetime(start)\n",
    "        end_date = pd.to_datetime(end)\n",
    "        \n",
    "        # Shade the intervention period\n",
    "        ax.axvspan(start_date, end_date, alpha=0.3, color=color, label=name)\n",
    "    \n",
    "    ax.set_title(f'{state} - Unemployment Claims with Intervention Periods', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('Unemployment Claims (thousands)', fontsize=12)\n",
    "    ax.legend(loc='upper right', fontsize=9, ncol=2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Results/intervention/intervention_periods_visualization.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: Results/intervention/intervention_periods_visualization.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================\n",
    "# FIT ARIMAX WITH INTERVENTIONS\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FITTING ARIMAX MODELS WITH INTERVENTION VARIABLES\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "results_list = []\n",
    "coefficient_list = []\n",
    "\n",
    "for state in states:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"STATE: {state}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get target variable\n",
    "    y_train = train_data[f'{state}_claims']\n",
    "    y_test = test_data[f'{state}_claims']\n",
    "    \n",
    "    # Exogenous variables: COVID features + intervention dummies\n",
    "    X_train = pd.concat([\n",
    "        train_data[[f'{state}_cases', f'{state}_deaths']],\n",
    "        train_interventions\n",
    "    ], axis=1)\n",
    "    \n",
    "    X_test = pd.concat([\n",
    "        test_data[[f'{state}_cases', f'{state}_deaths']],\n",
    "        test_interventions\n",
    "    ], axis=1)\n",
    "    \n",
    "    print(f\"\\nTarget: {state}_claims\")\n",
    "    print(f\"Exogenous variables ({X_train.shape[1]}):\")\n",
    "    print(f\"  - {state}_cases\")\n",
    "    print(f\"  - {state}_deaths\")\n",
    "    for intv in interventions.keys():\n",
    "        print(f\"  - {intv}\")\n",
    "    \n",
    "    try:\n",
    "        # Fit ARIMAX model\n",
    "        print(f\"\\nFitting ARIMAX(2,1,0) with {X_train.shape[1]} exogenous variables...\")\n",
    "        \n",
    "        model = SARIMAX(\n",
    "            y_train,\n",
    "            exog=X_train,\n",
    "            order=(2, 1, 0),\n",
    "            seasonal_order=(0, 0, 0, 0),\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False\n",
    "        )\n",
    "        \n",
    "        fitted = model.fit(disp=False, maxiter=200)\n",
    "        \n",
    "        print(\" Model fitted successfully\")\n",
    "        \n",
    "        # Generate forecast\n",
    "        forecast = fitted.forecast(steps=len(test_data), exog=X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_test, forecast)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, forecast))\n",
    "        r2 = r2_score(y_test, forecast)\n",
    "        \n",
    "        # Calculate MAPE safely\n",
    "        mask = y_test != 0\n",
    "        if mask.sum() > 0:\n",
    "            mape = np.mean(np.abs((y_test[mask] - forecast[mask]) / y_test[mask])) * 100\n",
    "        else:\n",
    "            mape = np.nan\n",
    "        \n",
    "        # Direction accuracy\n",
    "        actual_direction = np.sign(np.diff(y_test.values))\n",
    "        pred_direction = np.sign(np.diff(forecast.values))\n",
    "        direction_acc = np.mean(actual_direction == pred_direction) * 100\n",
    "        \n",
    "        # Store results\n",
    "        results_list.append({\n",
    "            'State': state,\n",
    "            'Model': 'ARIMAX_Intervention',\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R2': r2,\n",
    "            'MAPE': mape,\n",
    "            'Direction_Acc': direction_acc,\n",
    "            'AIC': fitted.aic,\n",
    "            'BIC': fitted.bic,\n",
    "            'Converged': fitted.mle_retvals['converged']\n",
    "        })\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RESULTS FOR {state}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"  MAE:                {mae:,.0f}\")\n",
    "        print(f\"  RMSE:               {rmse:,.0f}\")\n",
    "        print(f\"  MAPE:               {mape:.2f}%\")\n",
    "        print(f\"  R²:                 {r2:.3f}\")\n",
    "        print(f\"  Direction Accuracy: {direction_acc:.1f}%\")\n",
    "        print(f\"  AIC:                {fitted.aic:.0f}\")\n",
    "        print(f\"  BIC:                {fitted.bic:.0f}\")\n",
    "        print(f\"  Converged:          {fitted.mle_retvals['converged']}\")\n",
    "        \n",
    "        # Extract intervention coefficients\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"INTERVENTION COEFFICIENTS FOR {state}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        params = fitted.params\n",
    "        pvalues = fitted.pvalues\n",
    "        \n",
    "        for intv_name in interventions.keys():\n",
    "            if intv_name in params.index:\n",
    "                coef = params[intv_name]\n",
    "                pval = pvalues[intv_name]\n",
    "                significant = \"***\" if pval < 0.001 else \"**\" if pval < 0.01 else \"*\" if pval < 0.05 else \"\"\n",
    "                \n",
    "                print(f\"  {intv_name:20s}: {coef:10,.2f}  (p={pval:.4f}) {significant}\")\n",
    "                \n",
    "                coefficient_list.append({\n",
    "                    'State': state,\n",
    "                    'Intervention': intv_name,\n",
    "                    'Coefficient': coef,\n",
    "                    'P_Value': pval,\n",
    "                    'Significant': pval < 0.05\n",
    "                })\n",
    "        \n",
    "        # Save individual forecast\n",
    "        forecast_df = pd.DataFrame({\n",
    "            'date': test_data['date'],\n",
    "            'actual': y_test.values,\n",
    "            'forecast': forecast.values,\n",
    "            'error': y_test.values - forecast.values\n",
    "        })\n",
    "        forecast_df.to_csv(f'Results/intervention/forecast_{state}.csv', index=False)\n",
    "        print(f\"\\n Saved forecast: Results/intervention/forecast_{state}.csv\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n ERROR: Model failed for {state}\")\n",
    "        print(f\"   {str(e)}\")\n",
    "        \n",
    "        results_list.append({\n",
    "            'State': state,\n",
    "            'Model': 'ARIMAX_Intervention',\n",
    "            'MAE': np.nan,\n",
    "            'RMSE': np.nan,\n",
    "            'R2': np.nan,\n",
    "            'MAPE': np.nan,\n",
    "            'Direction_Acc': np.nan,\n",
    "            'AIC': np.nan,\n",
    "            'BIC': np.nan,\n",
    "            'Converged': False\n",
    "        })\n",
    "\n",
    "# ============================================\n",
    "# SAVE RESULTS\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SAVING RESULTS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Save main results\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df.to_csv('Results/intervention/arimax_intervention_results.csv', index=False)\n",
    "print(\" Saved: Results/intervention/arimax_intervention_results.csv\")\n",
    "\n",
    "# Save intervention coefficients\n",
    "if len(coefficient_list) > 0:\n",
    "    intervention_coefficients = pd.DataFrame(coefficient_list)\n",
    "    intervention_coefficients.to_csv('Results/intervention/intervention_coefficients.csv', index=False)\n",
    "    print(\" Saved: Results/intervention/intervention_coefficients.csv\")\n",
    "else:\n",
    "    print(\" No intervention coefficients to save\")\n",
    "\n",
    "# ============================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY - ARIMAX WITH INTERVENTIONS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(results_df[['State', 'MAE', 'RMSE', 'R2', 'Direction_Acc']].to_string(index=False))\n",
    "\n",
    "if len(coefficient_list) > 0:\n",
    "    print(f\"\\nIntervention Effects (Significant at p<0.05):\")\n",
    "    sig_interventions = intervention_coefficients[intervention_coefficients['Significant']]\n",
    "    \n",
    "    if len(sig_interventions) > 0:\n",
    "        print(sig_interventions[['State', 'Intervention', 'Coefficient', 'P_Value']].to_string(index=False))\n",
    "    else:\n",
    "        print(\"  No interventions were statistically significant\")\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZATION: FORECASTS\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CREATING FORECAST VISUALIZATIONS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "fig, axes = plt.subplots(len(states), 2, figsize=(16, 5*len(states)))\n",
    "\n",
    "if len(states) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for idx, state in enumerate(states):\n",
    "    # Load forecast for this state\n",
    "    try:\n",
    "        forecast_data = pd.read_csv(f'Results/intervention/forecast_{state}.csv')\n",
    "        forecast_data['date'] = pd.to_datetime(forecast_data['date'])\n",
    "        \n",
    "        # Left plot: Forecast vs Actual\n",
    "        ax1 = axes[idx, 0]\n",
    "        \n",
    "        # Plot training data (context)\n",
    "        ax1.plot(train_data['date'], train_data[f'{state}_claims'],\n",
    "                 color='gray', alpha=0.3, linewidth=1, label='Training Data')\n",
    "        \n",
    "        # Plot test actual\n",
    "        ax1.plot(forecast_data['date'], forecast_data['actual'],\n",
    "                 color='black', linewidth=2.5, marker='o', markersize=5,\n",
    "                 label='Actual', zorder=3)\n",
    "        \n",
    "        # Plot forecast\n",
    "        ax1.plot(forecast_data['date'], forecast_data['forecast'],\n",
    "                 color='red', linewidth=2.5, marker='s', markersize=4,\n",
    "                 linestyle='--', label='Forecast', alpha=0.8, zorder=2)\n",
    "        \n",
    "        # Split line\n",
    "        ax1.axvline(train_data['date'].max(), color='blue', \n",
    "                    linestyle='--', linewidth=2, alpha=0.5, label='Train/Test Split')\n",
    "        \n",
    "        # Get metrics for title\n",
    "        state_results = results_df[results_df['State'] == state].iloc[0]\n",
    "        \n",
    "        ax1.set_title(f'{state} - ARIMAX Intervention Forecast\\n' + \n",
    "                      f'MAE: {state_results[\"MAE\"]:,.0f} | R²: {state_results[\"R2\"]:.3f}',\n",
    "                      fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Date', fontsize=12)\n",
    "        ax1.set_ylabel('Unemployment Claims', fontsize=12)\n",
    "        ax1.legend(loc='best', fontsize=10)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Right plot: Errors\n",
    "        ax2 = axes[idx, 1]\n",
    "        \n",
    "        ax2.plot(forecast_data['date'], forecast_data['error'],\n",
    "                 color='purple', linewidth=2, marker='o', markersize=4)\n",
    "        ax2.axhline(0, color='black', linestyle='-', linewidth=1)\n",
    "        ax2.axhline(forecast_data['error'].mean(), color='red', linestyle='--',\n",
    "                    linewidth=2, label=f'Mean: {forecast_data[\"error\"].mean():,.0f}')\n",
    "        \n",
    "        std_error = forecast_data['error'].std()\n",
    "        ax2.fill_between(forecast_data['date'], -std_error, std_error,\n",
    "                         alpha=0.2, color='gray', label=f'±1σ: {std_error:,.0f}')\n",
    "        \n",
    "        ax2.set_title(f'{state} - Forecast Errors',\n",
    "                      fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Date', fontsize=12)\n",
    "        ax2.set_ylabel('Forecast Error', fontsize=12)\n",
    "        ax2.legend(loc='best', fontsize=10)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Could not plot {state}: {e}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Results/intervention/forecasts_visualization.png', dpi=300, bbox_inches='tight')\n",
    "print(\"    Saved: Results/intervention/forecasts_visualization.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZATION: COEFFICIENT HEATMAP\n",
    "# ============================================\n",
    "\n",
    "if len(coefficient_list) > 0:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CREATING COEFFICIENT HEATMAP\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Pivot for heatmap\n",
    "    coef_pivot = intervention_coefficients.pivot(\n",
    "        index='State',\n",
    "        columns='Intervention',\n",
    "        values='Coefficient'\n",
    "    )\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    import seaborn as sns\n",
    "    sns.heatmap(coef_pivot, annot=True, fmt='.0f', cmap='RdBu_r', \n",
    "                center=0, ax=ax, cbar_kws={'label': 'Coefficient Value'})\n",
    "    \n",
    "    ax.set_title('Intervention Coefficients by State\\n(Positive = Increased Claims, Negative = Decreased Claims)',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Intervention Period', fontsize=12)\n",
    "    ax.set_ylabel('State', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Results/intervention/coefficient_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"    Saved: Results/intervention/coefficient_heatmap.png\")\n",
    "    plt.close()\n",
    "\n",
    "# ============================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"INTERVENTION ANALYSIS COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\n Successfully analyzed {len(states)} states\")\n",
    "print(f\" Modeled {len(interventions)} intervention periods\")\n",
    "\n",
    "print(f\"\\n FILES CREATED:\")\n",
    "print(f\"    Results/intervention/arimax_intervention_results.csv\")\n",
    "print(f\"    Results/intervention/intervention_coefficients.csv\")\n",
    "print(f\"    Results/intervention/intervention_periods_visualization.png\")\n",
    "print(f\"    Results/intervention/forecasts_visualization.png\")\n",
    "print(f\"    Results/intervention/coefficient_heatmap.png\")\n",
    "for state in states:\n",
    "    print(f\"   Results/intervention/forecast_{state}.csv\")\n",
    "\n",
    "print(f\"\\n INTERPRETATION:\")\n",
    "print(f\"   • Positive coefficients → Intervention increased claims\")\n",
    "print(f\"   • Negative coefficients → Intervention decreased claims\")\n",
    "print(f\"   • Check p-values for statistical significance\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9f25a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "FinalFloridaCombinedData = pd.read_csv(\"FinalFloridaCombinedData.csv\")\n",
    "\n",
    "FinalGeorgiaCombinedData = pd.read_csv(\"Final_GA_CombinedData.csv\")\n",
    "\n",
    "FinalNewYorkCombinedData = pd.read_csv(\"Final_NY_CombinedData.csv\")\n",
    "\n",
    "\n",
    "cols_to_keep = [\"Date\", \"claims\", \"Confirmed\", \"Deaths\"]\n",
    "\n",
    "FinalFloridaCombinedData = FinalFloridaCombinedData[cols_to_keep]\n",
    "FinalGeorgiaCombinedData = FinalGeorgiaCombinedData[cols_to_keep]\n",
    "FinalNewYorkCombinedData = FinalNewYorkCombinedData[cols_to_keep]\n",
    "\n",
    "\n",
    "FinalFloridaCombinedData['Date'] = pd.to_datetime(FinalFloridaCombinedData['Date'])\n",
    "FinalGeorgiaCombinedData['Date'] = pd.to_datetime(FinalGeorgiaCombinedData['Date'])\n",
    "\n",
    "FinalNewYorkCombinedData['Date'] = pd.to_datetime(FinalNewYorkCombinedData['Date'])\n",
    "\n",
    "\n",
    "def create_summary_table(df, state_name):\n",
    "   \n",
    "    numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "    print(df.columns)\n",
    "\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    \n",
    "    print(df.columns)\n",
    "    summary = numeric_df.describe().T  \n",
    "    summary = summary[['count', 'mean', 'std', 'min', 'max']] \n",
    "    \n",
    "    summary = summary.reset_index()\n",
    "    summary.rename(columns={'index': 'Variable'}, inplace=True)\n",
    "    \n",
    "    start_date = df['Date'].min().strftime('%Y-%m-%d')\n",
    "    end_date = df['Date'].max().strftime('%Y-%m-%d')\n",
    "    \n",
    "    summary['State'] = state_name\n",
    "    summary['Time Period'] = f\"{start_date} to {end_date}\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate summaries\n",
    "fl_summary = create_summary_table(FinalFloridaCombinedData, 'Florida')\n",
    "ga_summary = create_summary_table(FinalGeorgiaCombinedData, 'Georgia')\n",
    "ny_summary = create_summary_table(FinalNewYorkCombinedData, 'New york')\n",
    "\n",
    "# Display \n",
    "fl_summary\n",
    "\n",
    "\n",
    "# to CSV files\n",
    "fl_summary.to_csv(\"Florida_Summary_Table.csv\", index=False)\n",
    "ga_summary.to_csv(\"Georgia_Summary_Table.csv\", index=False)\n",
    "ny_summary.to_csv(\"New_York_Summary_Table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ca093",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"Removes padding on the right to preserve causality.\n",
    "\n",
    "    In a temporal conv net we often use padding to keep the output length\n",
    "    the same as the input length. For causal convolutions, we pad only on\n",
    "    the left and then chomp off extra elements on the right.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chomp_size: int):\n",
    "        super().__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch, channels, length + chomp_size)\n",
    "        if self.chomp_size == 0:\n",
    "            return x\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"A single residual block in the Temporal Convolutional Network.\n",
    "\n",
    "    Each block has:\n",
    "      - dilated causal Conv1d\n",
    "      - weight normalization\n",
    "      - activation + dropout\n",
    "      - second Conv1d + activation + dropout\n",
    "      - residual connection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        dilation: int,\n",
    "        dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size - 1) * dilation\n",
    "\n",
    "        self.conv1 = nn.utils.weight_norm(\n",
    "            nn.Conv1d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                padding=padding,\n",
    "                dilation=dilation,\n",
    "            )\n",
    "        )\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.utils.weight_norm(\n",
    "            nn.Conv1d(\n",
    "                out_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                padding=padding,\n",
    "                dilation=dilation,\n",
    "            )\n",
    "        )\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # If in_channels != out_channels, use 1x1 conv to match dimensions\n",
    "        self.downsample = (\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "            if in_channels != out_channels\n",
    "            else None\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        for m in [self.conv1, self.conv2]:\n",
    "            nn.init.kaiming_normal_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        if isinstance(self.downsample, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(self.downsample.weight.data)\n",
    "            if self.downsample.bias is not None:\n",
    "                self.downsample.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.conv1(x)\n",
    "        out = self.chomp1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.chomp2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout2(out)\n",
    "\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    \"\"\"Stack of TemporalBlocks with exponentially increasing dilation.\n",
    "\n",
    "    Args:\n",
    "        num_inputs: number of input channels.\n",
    "        num_channels: list with the number of channels in each hidden layer.\n",
    "        kernel_size: convolution kernel size.\n",
    "        dropout: dropout rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_inputs: int,\n",
    "        num_channels: List[int],\n",
    "        kernel_size: int = 2,\n",
    "        dropout: float = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            in_ch = num_inputs if i == 0 else num_channels[i - 1]\n",
    "            out_ch = num_channels[i]\n",
    "            dilation = 2 ** i\n",
    "            layers.append(\n",
    "                TemporalBlock(\n",
    "                    in_channels=in_ch,\n",
    "                    out_channels=out_ch,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation=dilation,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "            )\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: tensor of shape (batch, channels, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            tensor of shape (batch, num_channels[-1], seq_len).\n",
    "        \"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class TCNForecaster(nn.Module):\n",
    "    \"\"\"Temporal Convolutional Network for sequence forecasting.\n",
    "\n",
    "    This model:\n",
    "      * encodes a history window with a TCN encoder\n",
    "      * takes the last time step's hidden state\n",
    "      * projects it to the forecast horizon with an MLP head.\n",
    "\n",
    "    Shapes:\n",
    "      - Input:  (batch, history_length, input_dim)\n",
    "      - Output: (batch, horizon, target_dim)\n",
    "\n",
    "    For univariate forecasting, input_dim = target_dim = 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        history_length: int,\n",
    "        horizon: int,\n",
    "        input_dim: int = 1,\n",
    "        target_dim: int = 1,\n",
    "        encoder_hidden_size: int = 128,\n",
    "        encoder_layers: int = 3,\n",
    "        kernel_size: int = 2,\n",
    "        dropout: float = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TCN expects (batch, channels, seq_len)\n",
    "        num_channels = [encoder_hidden_size] * encoder_layers\n",
    "        self.tcn = TemporalConvNet(\n",
    "            num_inputs=input_dim,\n",
    "            num_channels=num_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.history_length = history_length\n",
    "        self.horizon = horizon\n",
    "        self.input_dim = input_dim\n",
    "        self.target_dim = target_dim\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(num_channels[-1], encoder_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoder_hidden_size, horizon * target_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: tensor of shape (batch, history_length, input_dim)\n",
    "\n",
    "        Returns:\n",
    "            y_hat: tensor of shape (batch, horizon, target_dim)\n",
    "        \"\"\"\n",
    "        # Rearrange to (batch, channels=input_dim, seq_len=history_length)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        y_tcn = self.tcn(x)  # (batch, hidden, seq_len)\n",
    "        # Use representation at last time step\n",
    "        last_hidden = y_tcn[:, :, -1]  # (batch, hidden)\n",
    "\n",
    "        out = self.head(last_hidden)  # (batch, horizon * target_dim)\n",
    "        out = out.view(-1, self.horizon, self.target_dim)\n",
    "        return out\n",
    "\n",
    "\n",
    "class WindowedTimeSeriesDataset(Dataset):\n",
    "    \"\"\"Sliding-window dataset for supervised time-series forecasting.\n",
    "\n",
    "    Given a 1D series y[0...T-1], it builds samples:\n",
    "      - input:  y[i : i + history_length]\n",
    "      - target: y[i + history_length : i + history_length + horizon]\n",
    "\n",
    "    Args:\n",
    "        series: 1D array-like of floats.\n",
    "        history_length: number of past points used as input.\n",
    "        horizon: number of future points to predict.\n",
    "        stride: step between consecutive windows.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        series: np.ndarray,\n",
    "        history_length: int,\n",
    "        horizon: int,\n",
    "        stride: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        series = np.asarray(series, dtype=np.float32)\n",
    "        self.series = series\n",
    "        self.history_length = history_length\n",
    "        self.horizon = horizon\n",
    "        self.stride = stride\n",
    "\n",
    "        if series.ndim != 1:\n",
    "            raise ValueError(\"`series` must be 1D (univariate).\")\n",
    "\n",
    "        max_start = len(series) - history_length - horizon\n",
    "        if max_start < 0:\n",
    "            raise ValueError(\n",
    "                \"Series too short for the given history_length and horizon.\"\n",
    "            )\n",
    "        self.indices = np.arange(0, max_start + 1, stride, dtype=int)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        i = self.indices[idx]\n",
    "        x = self.series[i : i + self.history_length]\n",
    "        y = self.series[i + self.history_length : i + self.history_length + self.horizon]\n",
    "\n",
    "        # Add feature dimension = 1\n",
    "        x = torch.from_numpy(x).view(-1, 1)  # (history_length, 1)\n",
    "        y = torch.from_numpy(y).view(-1, 1)  # (horizon, 1)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "class MultiWindowedTimeSeriesDataset(Dataset):\n",
    "    \"\"\"Sliding-window dataset for *multivariate* time-series forecasting.\n",
    "\n",
    "    series: 2D array (time, features)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : (history_length, num_features)\n",
    "    y : (horizon,        num_features)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        series: np.ndarray,\n",
    "        history_length: int,\n",
    "        horizon: int,\n",
    "        stride: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        series = np.asarray(series, dtype=np.float32)\n",
    "        if series.ndim != 2:\n",
    "            raise ValueError(\"`series` must be 2D: (time, features).\")\n",
    "\n",
    "        self.series = series\n",
    "        self.history_length = history_length\n",
    "        self.horizon = horizon\n",
    "        self.stride = stride\n",
    "\n",
    "        max_start = len(series) - history_length - horizon\n",
    "        if max_start < 0:\n",
    "            raise ValueError(\n",
    "                \"Series too short for the given history_length and horizon.\"\n",
    "            )\n",
    "        self.indices = np.arange(0, max_start + 1, stride, dtype=int)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        i = self.indices[idx]\n",
    "        x = self.series[i : i + self.history_length, :]\n",
    "        y = self.series[i + self.history_length : i + self.history_length + self.horizon, :]\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    criterion: nn.Module,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        n += batch_size\n",
    "\n",
    "    return total_loss / n\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    criterion: nn.Module,\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        n += batch_size\n",
    "\n",
    "    return total_loss / n\n",
    "\n",
    "\n",
    "def fit_tcn(\n",
    "    series: np.ndarray,\n",
    "    history_length: int,\n",
    "    horizon: int,\n",
    "    batch_size: int = 32,\n",
    "    num_epochs: int = 50,\n",
    "    learning_rate: float = 1e-3,\n",
    "    val_ratio: float = 0.2,\n",
    "    encoder_hidden_size: int = 128,\n",
    "    encoder_layers: int = 3,\n",
    "    kernel_size: int = 2,\n",
    "    dropout: float = 0.2,\n",
    "    seed: int = 1,\n",
    ") -> Tuple[TCNForecaster, dict]:\n",
    "    \"\"\"Convenience function to train a TCNForecaster on a univariate series.\n",
    "\n",
    "    Splits the series into train/validation at the end, builds windowed\n",
    "    datasets, trains the model, and returns the fitted model plus a dict\n",
    "    with training history.\n",
    "\n",
    "    Returns:\n",
    "        model, history where history has keys:\n",
    "            - 'train_loss'\n",
    "            - 'val_loss'\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Train/val split on the raw series\n",
    "    n = len(series)\n",
    "    if n <= history_length + horizon:\n",
    "        raise ValueError(\"Series is too short for the given history_length and horizon.\")\n",
    "\n",
    "    split_idx = int(n * (1 - val_ratio))\n",
    "    train_series = series[:split_idx]\n",
    "    # Extend validation slice backward so it has enough history for windows\n",
    "    val_series = series[split_idx - history_length - horizon :]\n",
    "\n",
    "    train_ds = WindowedTimeSeriesDataset(train_series, history_length, horizon)\n",
    "    val_ds = WindowedTimeSeriesDataset(val_series, history_length, horizon)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = TCNForecaster(\n",
    "        history_length=history_length,\n",
    "        horizon=horizon,\n",
    "        input_dim=1,\n",
    "        target_dim=1,\n",
    "        encoder_hidden_size=encoder_hidden_size,\n",
    "        encoder_layers=encoder_layers,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout=dropout,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    best_val = math.inf\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device, criterion)\n",
    "        val_loss = evaluate_epoch(model, val_loader, device, criterion)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | train_loss={train_loss:.5f} | val_loss={val_loss:.5f}\"\n",
    "        )\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    history = {\"train_loss\": train_losses, \"val_loss\": val_losses}\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def fit_tcn_multivariate(\n",
    "    series_2d: np.ndarray,\n",
    "    history_length: int,\n",
    "    horizon: int,\n",
    "    batch_size: int = 32,\n",
    "    num_epochs: int = 50,\n",
    "    learning_rate: float = 1e-3,\n",
    "    val_ratio: float = 0.2,\n",
    "    encoder_hidden_size: int = 128,\n",
    "    encoder_layers: int = 3,\n",
    "    kernel_size: int = 2,\n",
    "    dropout: float = 0.2,\n",
    "    seed: int = 1,\n",
    "):\n",
    "    \"\"\"Train a TCNForecaster on a *multivariate* series.\n",
    "\n",
    "    series_2d: 2D array (time, features)\n",
    "\n",
    "    The model uses all features as inputs and predicts all of them jointly.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    series_2d = np.asarray(series_2d, dtype=np.float32)\n",
    "    if series_2d.ndim != 2:\n",
    "        raise ValueError(\"`series_2d` must be 2D: (time, features).\")\n",
    "\n",
    "    T, D = series_2d.shape\n",
    "    if T <= history_length + horizon:\n",
    "        raise ValueError(\"Series is too short for the given history_length and horizon.\")\n",
    "\n",
    "    # train/val split on the time axis\n",
    "    split_idx = int(T * (1 - val_ratio))\n",
    "    train_series = series_2d[:split_idx, :]\n",
    "    # extend val slice backward to have enough history for windows\n",
    "    val_series = series_2d[split_idx - history_length - horizon :, :]\n",
    "\n",
    "    train_ds = MultiWindowedTimeSeriesDataset(train_series, history_length, horizon)\n",
    "    val_ds = MultiWindowedTimeSeriesDataset(val_series, history_length, horizon)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = TCNForecaster(\n",
    "        history_length=history_length,\n",
    "        horizon=horizon,\n",
    "        input_dim=D,\n",
    "        target_dim=D,  # predict all features\n",
    "        encoder_hidden_size=encoder_hidden_size,\n",
    "        encoder_layers=encoder_layers,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout=dropout,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val = math.inf\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device, criterion)\n",
    "        val_loss = evaluate_epoch(model, val_loader, device, criterion)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        print(\n",
    "            f\"[Multi] Epoch {epoch:03d} | train_loss={train_loss:.5f} \"\n",
    "            f\"| val_loss={val_loss:.5f}\"\n",
    "        )\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    history = {\"train_loss\": train_losses, \"val_loss\": val_losses}\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def forecast(\n",
    "    model: TCNForecaster,\n",
    "    history: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate a forecast from a fitted model and a history window.\n",
    "\n",
    "    Args:\n",
    "        model: trained TCNForecaster.\n",
    "        history: 1D array-like of length == model.history_length.\n",
    "\n",
    "    Returns:\n",
    "        forecast array of shape (horizon,)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    history = np.asarray(history, dtype=np.float32)\n",
    "    if history.ndim != 1:\n",
    "        raise ValueError(\"`history` must be 1D.\")\n",
    "    if len(history) != model.history_length:\n",
    "        raise ValueError(\n",
    "            f\"`history` must have length {model.history_length}, \"\n",
    "            f\"got {len(history)}.\"\n",
    "        )\n",
    "\n",
    "    x = torch.from_numpy(history).view(1, -1, 1)  # (1, history_length, 1)\n",
    "    x = x.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(x)  # (1, horizon, 1)\n",
    "    return y_hat.cpu().numpy().reshape(-1)\n",
    "\n",
    "\n",
    "def forecast_multivariate(\n",
    "    model: TCNForecaster,\n",
    "    history_2d: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate a forecast for a *multivariate* series.\n",
    "\n",
    "    history_2d: 2D array of shape (history_length, input_dim)\n",
    "    Returns: array of shape (horizon, target_dim)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    history_2d = np.asarray(history_2d, dtype=np.float32)\n",
    "    if history_2d.ndim != 2:\n",
    "        raise ValueError(\"`history_2d` must be 2D (time, features).\")\n",
    "    if history_2d.shape[0] != model.history_length:\n",
    "        raise ValueError(\n",
    "            f\"`history_2d` must have length {model.history_length}, \"\n",
    "            f\"got {history_2d.shape[0]}.\"\n",
    "        )\n",
    "\n",
    "    # shape: (1, history_length, input_dim)\n",
    "    x = torch.from_numpy(history_2d).unsqueeze(0)\n",
    "    x = x.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(x)  # (1, horizon, target_dim)\n",
    "\n",
    "    # return (horizon, target_dim)\n",
    "    return y_hat.cpu().numpy()[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage on a dummy sine-wave series.\n",
    "    timesteps = np.linspace(0, 100, 1000, dtype=np.float32)\n",
    "    series = np.sin(0.2 * timesteps) + 0.1 * np.random.randn(len(timesteps)).astype(\n",
    "        np.float32\n",
    "    )\n",
    "\n",
    "    HISTORY = 48\n",
    "    HORIZON = 12\n",
    "\n",
    "    model, history = fit_tcn(\n",
    "        series,\n",
    "        history_length=HISTORY,\n",
    "        horizon=HORIZON,\n",
    "        batch_size=32,\n",
    "        num_epochs=10,\n",
    "        learning_rate=1e-3,\n",
    "        encoder_hidden_size=128,\n",
    "        encoder_layers=3,\n",
    "        kernel_size=3,\n",
    "        dropout=0.1,\n",
    "    )\n",
    "\n",
    "    # Take the last HISTORY points as context and forecast the next HORIZON points.\n",
    "    ctx = series[-HISTORY:]\n",
    "    y_forecast = forecast(model, ctx)\n",
    "\n",
    "    print(\"Last observed values:\", ctx[-5:])\n",
    "    print(\"First 5-step forecast:\", y_forecast[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f62bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TCN UNIVARIATE FLORIDA\n",
    "\n",
    "# Ahmed's model\n",
    "\n",
    "\n",
    "# Config\n",
    "HISTORY = 28      # how many past days the model sees\n",
    "HORIZON = 7       # how many days ahead we forecast\n",
    "\n",
    "# read input data\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(__file__))\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, \"FinalFloridaCombinedData.csv\")\n",
    "print(\"Reading:\", DATA_PATH)\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"CSV columns:\", df.columns.tolist())\n",
    "for date_col in [\"date\", \"Date\"]:\n",
    "    if date_col in df.columns:\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "        df = df.set_index(date_col).sort_index()\n",
    "        break\n",
    "\n",
    "CLAIMS_COL = \"claims\"\n",
    "CASES_COL  = \"Confirmed\"\n",
    "DEATHS_COL = \"Deaths\"\n",
    "\n",
    "claims = df[CLAIMS_COL].astype(np.float32).values\n",
    "cases  = df[CASES_COL].astype(np.float32).values\n",
    "deaths = df[DEATHS_COL].astype(np.float32).values\n",
    "\n",
    "# Train two separate TCNs: one for claims, one for confirmed cases\n",
    "claims_train = claims[:-HORIZON]\n",
    "cases_train  = cases[:-HORIZON]\n",
    "deaths_train = deaths[:-HORIZON]\n",
    "\n",
    "print(\"\\nTraining TCN for unemployment claims\")\n",
    "model_claims, hist_claims = fit_tcn(\n",
    "    claims_train,\n",
    "    history_length=HISTORY,\n",
    "    horizon=HORIZON,\n",
    "    batch_size=32,\n",
    "    num_epochs=30,\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining TCN for confirmed COVID-19 cases\")\n",
    "model_cases, hist_cases = fit_tcn(\n",
    "    cases_train,\n",
    "    history_length=HISTORY,\n",
    "    horizon=HORIZON,\n",
    "    batch_size=32,\n",
    "    num_epochs=30,\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining TCN for COVID-19 deaths\")\n",
    "model_deaths, hist_deaths = fit_tcn(\n",
    "    deaths_train,\n",
    "    history_length=HISTORY,\n",
    "    horizon=HORIZON,\n",
    "    batch_size=32,\n",
    "    num_epochs=30,\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "\n",
    "# Build context windows to compare forecast vs actual on those held-out days\n",
    "ctx_claims = claims[-(HISTORY + HORIZON) : -HORIZON]\n",
    "ctx_cases  = cases[-(HISTORY + HORIZON) : -HORIZON]\n",
    "ctx_deaths = deaths[-(HISTORY + HORIZON) : -HORIZON]\n",
    "\n",
    "pred_claims = forecast(model_claims, ctx_claims)\n",
    "pred_cases  = forecast(model_cases, ctx_cases)\n",
    "pred_deaths = forecast(model_deaths, ctx_deaths)\n",
    "\n",
    "# Ground truth for those last HORIZON days\n",
    "future_dates = df.index[-HORIZON:]\n",
    "true_claims  = claims[-HORIZON:]\n",
    "true_cases   = cases[-HORIZON:]\n",
    "true_deaths  = deaths[-HORIZON:]\n",
    "\n",
    "# Plot: Claims\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(future_dates, true_claims, label=\"Actual claims\")\n",
    "plt.plot(future_dates, pred_claims, \"--\", label=\"TCN forecast\")\n",
    "plt.title(\"Florida – Initial Unemployment Claims (TCN forecast)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Claims\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot: Confirmed cases\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(future_dates, true_cases, label=\"Actual confirmed cases\")\n",
    "plt.plot(future_dates, pred_cases, \"--\", label=\"TCN forecast\")\n",
    "plt.title(\"Florida – Confirmed COVID-19 Cases (TCN forecast)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cases\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot: deaths\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(future_dates, true_deaths, label=\"Actual deaths\")\n",
    "plt.plot(future_dates, pred_deaths, \"--\", label=\"TCN forecast\")\n",
    "plt.title(\"Florida – COVID-19 Deaths (TCN forecast)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91239f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOFORMER MODEL\n",
    "\n",
    "\n",
    "# Package Versions Used\n",
    "# neuralforecast: 1.7.4\n",
    "# utilsforecast: 0.2.14\n",
    "# Python: 3.10.19\n",
    "\n",
    "#We Did a grid search using COLAB T4 GPU\n",
    "\n",
    "\n",
    "def evaluate_performance(true, pred, model_name=\"Model\"):\n",
    "    mspe = mean_squared_error(true, pred)\n",
    "    mae = mean_absolute_error(true, pred)\n",
    "    mape = np.mean(np.abs((true - pred) / (true + 1e-6)))\n",
    "    pm = (np.sum((true - pred) ** 2) / np.sum((true - np.mean(true)) ** 2))\n",
    "\n",
    "    print(f\"=== {model_name} Performance ===\")\n",
    "    print(f\"MSPE: {mspe:.4f}\")\n",
    "    print(f\"MAE:  {mae:.4f}\")\n",
    "    print(f\"MAPE: {mape:.4f}\")\n",
    "    print(f\"PM:   {pm:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "    return mspe, mae, mape, pm\n",
    "\n",
    "FinalFloridaCombinedData = pd.read_csv(\"Final_GA_CombinedData.csv\")\n",
    "\n",
    "\n",
    "df = FinalFloridaCombinedData.copy()\n",
    "df['ds'] = df.index\n",
    "df['unique_id'] = 'series_1'\n",
    "df = df.rename(columns=lambda x: x.strip())  \n",
    "try:\n",
    "    df = df.drop(columns=['Unnamed: 0'])  \n",
    "except:\n",
    "    x =1 \n",
    "\n",
    "df['ds'] = pd.to_datetime(df['Date'])\n",
    "df = df.drop(columns=['Date'])\n",
    "df['ds'] = pd.to_datetime(df['ds'].dt.date)\n",
    "\n",
    "# Sometimes the first day is excluded; shift +1 day\n",
    "df['ds'] = pd.to_datetime(df['ds']) + pd.to_timedelta(1, unit='D')\n",
    "\n",
    "# Target column claims:\n",
    "#df = df.rename(columns={'claims': 'y'})\n",
    "\n",
    "# Target column 'Confirmed'\n",
    "#df = df.rename(columns={'Confirmed': 'y'})\n",
    "\n",
    "# Target column 'Deaths'\n",
    "df = df.rename(columns={'Deaths': 'y'})\n",
    "\n",
    "\n",
    "df = df.drop(columns=['Active'])\n",
    "# FUTURE EXOGENOUS VARIABLES\n",
    "futr_cols = ['Deaths', 'Confirmed']  \n",
    "\n",
    "\n",
    "n = len(df)\n",
    "train = df.iloc[:int(n*0.7)]\n",
    "val   = df.iloc[int(n*0.7):int(n*0.85)]\n",
    "test  = df.iloc[int(n*0.85):]\n",
    "\n",
    "print(\"Train:\", train.shape)\n",
    "print(\"Val:\",   val.shape)\n",
    "print(\"Test:\",  test.shape)\n",
    "\n",
    "train.head()\n",
    "\n",
    "\n",
    "\n",
    "# Hyper parameter Check \n",
    "\n",
    "\"\"\"\n",
    "hidden_sizes = [8, 16, 32, 64, 128]\n",
    "conv_hidden_sizes = [16, 32, 64, 128, 256]\n",
    "learning_rates = [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]\n",
    "n_heads = [1, 2, 4, 8, 16]\n",
    "encoder_layers = [1, 2, 3, 4, 6]\n",
    "decoder_layers = [1, 2, 3, 4, 6]\n",
    "\"\"\"\n",
    "\n",
    "hidden_sizes      = [16, 64]\n",
    "conv_hidden_sizes = [32, 128]\n",
    "learning_rates    = [1e-3, 1e-4]\n",
    "n_heads           = [2, 4]\n",
    "encoder_layers    = [2, 4]\n",
    "decoder_layers    = [2, 4]\n",
    "\n",
    "top_models = [] \n",
    "\n",
    "\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    for conv_hidden_size in conv_hidden_sizes:\n",
    "        for lr in learning_rates:\n",
    "            for n_head in n_heads:\n",
    "                for enc_layers in encoder_layers:\n",
    "                    for dec_layers in decoder_layers:\n",
    "                        model = Autoformer(\n",
    "                            h=1,                      \n",
    "                            input_size=36,             \n",
    "                            hidden_size=hidden_size,\n",
    "                            conv_hidden_size=conv_hidden_size,\n",
    "                            n_head=n_head,\n",
    "                            encoder_layers=enc_layers,\n",
    "                            decoder_layers=dec_layers,\n",
    "                            loss=MAE(),\n",
    "                            \n",
    "                            futr_exog_list=None,   \n",
    "                            stat_exog_list=None,\n",
    "                            \n",
    "                            scaler_type='robust',\n",
    "                            learning_rate=lr,\n",
    "                            max_steps=200,\n",
    "                            val_check_steps=50,\n",
    "                            early_stop_patience_steps=2,\n",
    "                            enable_progress_bar=False,\n",
    "                            enable_model_summary=False,\n",
    "\n",
    "                                )\n",
    "\n",
    "\n",
    "                        nf = NeuralForecast(\n",
    "                            models=[model],\n",
    "                            freq='W'\n",
    "                        )\n",
    "\n",
    "                        nf.fit(df=train, val_size=len(val))\n",
    "\n",
    "                        val_pred = nf.predict(df=val)\n",
    "\n",
    "                        val_pred.head()\n",
    "\n",
    "                        rolling_preds = []\n",
    "\n",
    "                        for i in range(val.shape[0]):\n",
    "                            # data available up to this point\n",
    "                            df_until_now = pd.concat([train, val.iloc[:i]], axis=0)\n",
    "\n",
    "                            # forecast 1 step ahead\n",
    "                            pred_i = nf.predict(df=df_until_now)\n",
    "\n",
    "                            # store prediction (align with actual val timestamp)\n",
    "                            pred_i['ds'] = val.iloc[i]['ds']\n",
    "                            rolling_preds.append(pred_i)\n",
    "\n",
    "                        rolling_preds = pd.concat(rolling_preds).reset_index(drop=True)\n",
    "\n",
    "\n",
    "                        val_plot = val.merge(\n",
    "                            rolling_preds[['ds', 'Autoformer']],\n",
    "                            on='ds',\n",
    "                            how='left'\n",
    "                        )\n",
    "\n",
    "                        val_plot\n",
    "\n",
    "                        model_name = (\n",
    "                            f\"AF_h{hidden_size}\"\n",
    "                            f\"_c{conv_hidden_size}\"\n",
    "                            f\"_lr{lr}\"\n",
    "                            f\"_hd{n_head}\"\n",
    "                            f\"_enc{enc_layers}\"\n",
    "                            f\"_dec{dec_layers}\"\n",
    "                        )\n",
    "\n",
    "                        #evaluate_performance(val_plot['y'],val_plot['Autoformer'], model_name=\"AutoFormer\")\n",
    "                        mspe, mae, mape, pm = evaluate_performance(\n",
    "                            val_plot['y'],\n",
    "                            val_plot['Autoformer'],\n",
    "                            model_name= model_name\n",
    "                        )\n",
    "\n",
    "                        result = {\n",
    "                            'hidden_size': hidden_size,\n",
    "                            'conv_hidden_size': conv_hidden_size,\n",
    "                            'lr': lr,\n",
    "                            'n_head': n_head,\n",
    "                            'enc_layers' :enc_layers,\n",
    "                            'dec_layers' : dec_layers,\n",
    "                            'mspe': mspe,\n",
    "                            'mae': mae,\n",
    "                            'mape': mape,\n",
    "                            'pm': pm\n",
    "                        }\n",
    "\n",
    "                        top_models.append(result)\n",
    "\n",
    "                        \n",
    "# Sort by MAE and keep only top 3\n",
    "top_models = sorted(top_models, key=lambda x: x['mae'])[:3]\n",
    "print(\"\\n=== TOP 3 AUTOFORMER MODELS BY MAE ===\")\n",
    "\n",
    "for rank, model in enumerate(top_models, start=1):\n",
    "    print(f\"\\n# {rank}\")\n",
    "    for k, v in model.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "=== TOP 3 AUTOFORMER MODELS BY MAE ===\n",
    "\n",
    "# 1\n",
    "hidden_size: 16\n",
    "conv_hidden_size: 128\n",
    "lr: 0.0001\n",
    "n_head: 4\n",
    "enc_layers: 4\n",
    "dec_layers: 4\n",
    "mspe: 1827463.5054851745\n",
    "mae: 1087.5776685631793\n",
    "mape: 0.17673068714031312\n",
    "pm: 2.2589641080201557\n",
    "\n",
    "# 2\n",
    "hidden_size: 16\n",
    "conv_hidden_size: 128\n",
    "lr: 0.0001\n",
    "n_head: 2\n",
    "enc_layers: 4\n",
    "dec_layers: 4\n",
    "mspe: 1835138.4781292807\n",
    "mae: 1094.8162470278533\n",
    "mape: 0.17799333112702048\n",
    "pm: 2.268451294867408\n",
    "\n",
    "# 3\n",
    "hidden_size: 64\n",
    "conv_hidden_size: 128\n",
    "lr: 0.0001\n",
    "n_head: 2\n",
    "enc_layers: 2\n",
    "dec_layers: 2\n",
    "mspe: 2176483.54390021\n",
    "mae: 1216.295749830163\n",
    "mape: 0.19870637896432275\n",
    "pm: 2.6903947425542567\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Predict on test set \n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 16\n",
    "conv_hidden_size = 128\n",
    "lr = 0.0001\n",
    "n_head = 4\n",
    "enc_layers = 4\n",
    "dec_layers = 4\n",
    "\n",
    "# Define Model\n",
    "model = Autoformer(\n",
    "    h=1,\n",
    "    input_size=36,\n",
    "    hidden_size=hidden_size,\n",
    "    conv_hidden_size=conv_hidden_size,\n",
    "    n_head=n_head,\n",
    "    encoder_layers=enc_layers,\n",
    "    decoder_layers=dec_layers,\n",
    "    loss=MAE(),\n",
    "    futr_exog_list=None,\n",
    "    stat_exog_list=None,\n",
    "    \n",
    "    #hist_exog_list = ,\n",
    "    scaler_type='robust',\n",
    "    learning_rate=lr,\n",
    "    max_steps=200,\n",
    "    val_check_steps=50,\n",
    "    early_stop_patience_steps=2,\n",
    "    enable_progress_bar=False,\n",
    "    enable_model_summary=False,\n",
    ")\n",
    "\n",
    "nf = NeuralForecast(models=[model], freq='W')\n",
    "nf.fit(df=train, val_size=len(val))  \n",
    "\n",
    "rolling_preds = []\n",
    "\n",
    "for i in range(test.shape[0]):\n",
    "    hist_df = pd.concat([train, val, test.iloc[:i]], axis=0)\n",
    "\n",
    "    pred_i = nf.predict(df=hist_df)\n",
    "    pred_i['ds'] = test.iloc[i]['ds']  \n",
    "\n",
    "    rolling_preds.append(pred_i)\n",
    "\n",
    "rolling_preds = pd.concat(rolling_preds).reset_index(drop=True)\n",
    "\n",
    "# Merge Predictions with TEST Truth\n",
    "test_forecast = test.merge(\n",
    "    rolling_preds[['ds', 'Autoformer']],\n",
    "    on='ds',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "model_name = (\n",
    "    f\"AF_h{hidden_size}\"\n",
    "    f\"_c{conv_hidden_size}\"\n",
    "    f\"_lr{lr}\"\n",
    "    f\"_hd{n_head}\"\n",
    "    f\"_enc{enc_layers}\"\n",
    "    f\"_dec{dec_layers}\"\n",
    ")\n",
    "\n",
    "# Evaluate ON TEST ONLY\n",
    "mspe, mae, mape, pm = evaluate_performance(\n",
    "    test_forecast['y'],\n",
    "    test_forecast['Autoformer'],\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "print(model_name)\n",
    "print(\"MSPE:\", mspe)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MAPE:\", mape)\n",
    "print(\"PM:\", pm)\n",
    "\n",
    "test_forecast.head()\n",
    "\n",
    "\n",
    "# ================= PLOT: LAST TRAIN + VAL + TEST =================\n",
    "last_n_train = 30 \n",
    "\n",
    "train_zoom = train.iloc[-last_n_train:]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Actual segments\n",
    "plt.plot(train_zoom['ds'], train_zoom['y'], label='Train (Tail)', alpha=0.8)\n",
    "plt.plot(val['ds'], val['y'], label='Validation', alpha=0.8)\n",
    "plt.plot(test['ds'], test['y'], label='Test Actual', alpha=0.8)\n",
    "\n",
    "# Forecast\n",
    "plt.plot(test_forecast['ds'], test_forecast['Autoformer'],\n",
    "         label='Forecast (Test)',\n",
    "         linewidth=2)\n",
    "\n",
    "\n",
    "# Confidence Intervals \n",
    "if 'Autoformer-lo-90' in rolling_preds.columns:\n",
    "    merged_ci = test.merge(\n",
    "        rolling_preds[['ds', 'Autoformer-lo-90', 'Autoformer-hi-90']],\n",
    "        on='ds', how='left'\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        merged_ci['ds'],\n",
    "        merged_ci['Autoformer-lo-90'],\n",
    "        merged_ci['Autoformer-hi-90'],\n",
    "        alpha=0.2, label='90% CI'\n",
    "    )\n",
    "\n",
    "plt.axvline(val['ds'].iloc[0], linestyle='--', color='gray', alpha=0.8)\n",
    "plt.axvline(test['ds'].iloc[0], linestyle='--', color='gray', alpha=0.8)\n",
    "\n",
    "plt.title(f'Train Tail + Val + Test Forecast\\n{model_name}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Claims')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "#FOR CONFIRMED CASES\n",
    "# ================= PLOT: LAST TRAIN + VAL + TEST =================\n",
    "last_n_train = 30\n",
    "train_zoom = train.iloc[-last_n_train:]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Actual segments\n",
    "plt.plot(train_zoom['ds'], train_zoom['y'], label='Train (Tail)', alpha=0.8)\n",
    "plt.plot(val['ds'], val['y'], label='Validation', alpha=0.8)\n",
    "plt.plot(test['ds'], test['y'], label='Test Actual', alpha=0.8)\n",
    "\n",
    "# Forecast\n",
    "plt.plot(test_forecast['ds'], test_forecast['Autoformer'],\n",
    "         label='Forecast (Test)', linewidth=2)\n",
    "\n",
    "# Confidence Intervals\n",
    "if 'Autoformer-lo-90' in rolling_preds.columns:\n",
    "    merged_ci = test.merge(\n",
    "        rolling_preds[['ds', 'Autoformer-lo-90', 'Autoformer-hi-90']],\n",
    "        on='ds', how='left'\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        merged_ci['ds'],\n",
    "        merged_ci['Autoformer-lo-90'],\n",
    "        merged_ci['Autoformer-hi-90'],\n",
    "        alpha=0.2, label='90% CI'\n",
    "    )\n",
    "\n",
    "plt.axvline(val['ds'].iloc[0], linestyle='--', color='gray', alpha=0.8)\n",
    "plt.axvline(test['ds'].iloc[0], linestyle='--', color='gray', alpha=0.8)\n",
    "\n",
    "# Updated labels\n",
    "plt.title(f'Confirmed Cases: Train Tail + Validation + Test Forecast\\n{model_name}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Confirmed Cases')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# FOR DEATHS\n",
    "# ================= PLOT: LAST TRAIN + VAL + TEST (Deaths) =================\n",
    "last_n_train = 30\n",
    "train_zoom = train.iloc[-last_n_train:]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Actual segments\n",
    "plt.plot(train_zoom['ds'], train_zoom['y'], label='Train Deaths (Tail)', alpha=0.8)\n",
    "plt.plot(val['ds'], val['y'], label='Validation Deaths', alpha=0.8)\n",
    "plt.plot(test['ds'], test['y'], label='Test Actual Deaths', alpha=0.8)\n",
    "\n",
    "# Forecast\n",
    "plt.plot(test_forecast['ds'], test_forecast['Autoformer'],\n",
    "         label='Forecasted Deaths (Test)', linewidth=2)\n",
    "\n",
    "# Confidence Intervals\n",
    "if 'Autoformer-lo-90' in rolling_preds.columns:\n",
    "    merged_ci = test.merge(\n",
    "        rolling_preds[['ds', 'Autoformer-lo-90', 'Autoformer-hi-90']],\n",
    "        on='ds', how='left'\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        merged_ci['ds'],\n",
    "        merged_ci['Autoformer-lo-90'],\n",
    "        merged_ci['Autoformer-hi-90'],\n",
    "        alpha=0.2, label='90% CI (Deaths)'\n",
    "    )\n",
    "\n",
    "plt.axvline(val['ds'].iloc[0], linestyle='--', color='gray', alpha=0.8)\n",
    "plt.axvline(test['ds'].iloc[0], linestyle='--', color='gray', alpha=0.8)\n",
    "\n",
    "# Updated labels for Deaths\n",
    "plt.title(f'Deaths: Train Tail + Validation + Test Forecast\\n{model_name}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Deaths')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
